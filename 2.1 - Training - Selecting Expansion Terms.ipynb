{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREAMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cfg\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "import cPickle\n",
    "import heapq\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats\n",
    "import xgboost\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import progress_bar as pb\n",
    "import feature_extraction as fe\n",
    "\n",
    "import efficient_query_expansion.index_cache as index_cache\n",
    "from collection_stats.collection_stats_restricted import PyCollectionStatsRestricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "idx_cache_file_path = cfg.tmp_dir + \"index_cache_dump.bin\"\n",
    "if os.path.isfile(idx_cache_file_path):\n",
    "    idx_cache = index_cache.IndexCache.load(idx_cache_file_path)\n",
    "    print \"Found {} entries in cache\".format(len(idx_cache))\n",
    "else:\n",
    "    idx_cache = index_cache.IndexCache(host=\"127.0.0.1\", port=9001)\n",
    "    print \"Cache file not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cdef extern from \"math.h\":\n",
    "    double sqrt(double m)\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "cdef set _sets_union(set_iterator):\n",
    "    res = set()\n",
    "    for set_it in set_iterator:\n",
    "        res.update(set_it)\n",
    "    return res    \n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "cdef set _sets_intersection(set_iterator):\n",
    "    res = None\n",
    "    for set_it in set_iterator:\n",
    "        if res is None:\n",
    "            res = set_it\n",
    "        else:\n",
    "            res.intersection_update(set_it)\n",
    "    return res\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "def c_get_query_occurrences(query_repr, dict word_to_occurrence_set):\n",
    "    return (\n",
    "        _sets_union(\n",
    "            _sets_intersection(\n",
    "                _sets_union(\n",
    "                    word_to_occurrence_set[word_and_tags[0]]\n",
    "                    for word_and_tags in synset\n",
    "                )\n",
    "                for synset in and_query\n",
    "            )\n",
    "            for and_query in query_repr\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "def c_get_num_match(dict qid_to_query, qid_set, dict qid_to_word_to_occurrence_set):\n",
    "    if qid_set is None:\n",
    "        qid_set = qid_to_query.keys()\n",
    "    else:\n",
    "        qid_set = set(qid_set)\n",
    "\n",
    "    return dict(\n",
    "        (qid, len(c_get_query_occurrences(qid_to_query[qid], qid_to_word_to_occurrence_set[qid])))\n",
    "        for qid in qid_set\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def query_terms_iterator(query_repr, only_positions=None):\n",
    "    if only_positions is None:\n",
    "        abs_pos = -1\n",
    "        and_pos = -1\n",
    "        for and_query in query_repr:\n",
    "            and_pos +=1\n",
    "            syn_pos = -1\n",
    "            for synset in and_query:\n",
    "                syn_pos += 1\n",
    "                term_pos = -1\n",
    "                for term_tags in synset:\n",
    "                    abs_pos += 1\n",
    "                    term_pos += 1\n",
    "                    yield (abs_pos, and_pos, syn_pos, term_pos, term_tags)\n",
    "    else:\n",
    "        assert hasattr(only_positions, \"__iter__\")\n",
    "        only_positions = sorted(only_positions)\n",
    "        assert len(only_positions) > 0\n",
    "        i_stop = len(only_positions)\n",
    "        i = 0\n",
    "\n",
    "        abs_pos = -1\n",
    "        and_pos = -1\n",
    "        for and_query in query_repr:\n",
    "            and_pos +=1\n",
    "            syn_pos = -1\n",
    "            for synset in and_query:\n",
    "                syn_pos += 1\n",
    "                term_pos = -1\n",
    "                if only_positions[i] > abs_pos + len(synset):\n",
    "                    abs_pos += len(synset)\n",
    "                    continue\n",
    "                for term_tags in synset:\n",
    "                    abs_pos += 1\n",
    "                    term_pos += 1\n",
    "                    if only_positions[i] == abs_pos:\n",
    "                        yield (abs_pos, and_pos, syn_pos, term_pos, term_tags)\n",
    "                        i += 1\n",
    "                        if i >= i_stop:\n",
    "                            return\n",
    "        raise Exception(\"One of the positions ({}) is out of the query\".format(only_positions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_step_function(step, fun=None):\n",
    "    assert isinstance(step, float) and step >= 0\n",
    "    assert fun is None or (hasattr(fun, \"__call__\") and fun(0) == 1)\n",
    "\n",
    "    if fun is None:\n",
    "        return lambda v: 1 if v <= step else 0.0\n",
    "    else:\n",
    "        return lambda v: 1 if v <= step else fun(v-step)\n",
    "\n",
    "def get_lin_decay_function(slope):\n",
    "    assert isinstance(slope, float) and slope <= 0\n",
    "    return lambda v: 1 + slope * v\n",
    "\n",
    "def get_exp_decay_function(alpha):\n",
    "    assert isinstance(alpha, float) and alpha <= 0\n",
    "    return lambda v: math.exp(alpha * v)\n",
    "\n",
    "def get_line_decay_slope_from_step(step, ratio_step):\n",
    "    assert isinstance(step, float) and step > 0\n",
    "    assert isinstance(ratio_step, float) and ratio_step > 0\n",
    "    return -1.0 / (step * ratio_step)\n",
    "\n",
    "def get_exp_decay_alpha_from_step(step, ratio_step):\n",
    "    assert isinstance(step, float) and step > 0\n",
    "    assert isinstance(ratio_step, float) and ratio_step > 0\n",
    "    return -2.0 / (step * ratio_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def harmonic_mean(v1, v2, v1_weight=0.5):\n",
    "    return (v1 * v2) / ((v1_weight * v2 + (1.0 - v1_weight) * v1) or 1.0)\n",
    "\n",
    "def get_perf_to_harmonic_mean_function(fun1, fun2, fun1_weight=0.5):\n",
    "    assert isinstance(fun1_weight, float) and 0 <= fun1_weight <= 1\n",
    "    return lambda perf: harmonic_mean(fun1(perf), fun2(perf), fun1_weight)\n",
    "\n",
    "def perf_to_recall(perf):\n",
    "    return 1.0 * perf.num_rel_ret / perf.num_rel\n",
    "\n",
    "def get_perf_to_exe_time_function(fun):\n",
    "    return lambda perf: fun(perf.exe_time)\n",
    "\n",
    "def get_perf_to_my_eet_function(step, step_ratio, effectivity_weight=0.5):\n",
    "    assert isinstance(step, float) and step > 0\n",
    "    assert isinstance(step_ratio, float) and step_ratio >= 0\n",
    "\n",
    "    effectivity_fun = perf_to_recall\n",
    "    efficiency_fun = get_perf_to_exe_time_function(get_step_function(step, fun=(None if step_ratio == 0 else get_exp_decay_function(get_exp_decay_alpha_from_step(step, step_ratio)))))\n",
    "\n",
    "    return get_perf_to_harmonic_mean_function(effectivity_fun, efficiency_fun, effectivity_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def jsonConvertKeys(constructor):\n",
    "    return lambda x: {constructor(k):v for k,v in x.items()}\n",
    "\n",
    "qid_to_query = json.load(open(cfg.training_dir + \"qid_to_query.json\", \"r\"), object_hook=jsonConvertKeys(int))\n",
    "qid_to_docid_list = json.load(open(cfg.training_dir + \"qid_to_docid_list.json\", \"r\"), object_hook=jsonConvertKeys(int))\n",
    "\n",
    "assert len(qid_to_query) == len(qid_to_docid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qid_to_base_query = cPickle.load(open(cfg.processed_dir + \"qid_to_base_query.pickle\"))\n",
    "qid_to_candidates = cPickle.load(open(cfg.processed_dir + \"qid_to_candidates.pickle\"))\n",
    "\n",
    "assert set(qid_to_base_query.keys()) == set(qid_to_candidates.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qid_to_num_candidates = dict(\n",
    "    (qid, sum(\n",
    "        len(synset)\n",
    "        for and_query in candidates  # the query is the OR composition of different AND_QUERIES\n",
    "        for synset in and_query  # the AND_QUERY is the AND composition of different SYNSET\n",
    "    ))\n",
    "    for qid, candidates in qid_to_candidates.iteritems()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "queries_with_recall_improvement = cPickle.load(open(cfg.processed_dir + \"queries_with_recall_improvement.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qid_to_word_to_occurrence_set = cPickle.load(open(cfg.processed_dir + \"qid_to_word_to_occurrence_set.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_num_match(qid_to_query, qid_set=None):\n",
    "    global qid_to_word_to_occurrence_set\n",
    "\n",
    "    return c_get_num_match(qid_to_query, qid_set, qid_to_word_to_occurrence_set)\n",
    "\n",
    "def get_num_match_query(qid, query_repr):\n",
    "    global qid_to_word_to_occurrence_set\n",
    "\n",
    "    return len(c_get_query_occurrences(query_repr, qid_to_word_to_occurrence_set[qid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checks\n",
    "for qid, candidates in qid_to_candidates.iteritems():\n",
    "    base_query = qid_to_base_query[qid]\n",
    "\n",
    "    # check the synonyms\n",
    "    if any(syn.strip() == \"\"\n",
    "           for and_query in candidates\n",
    "           for synset in and_query\n",
    "           for syn, tag in synset):\n",
    "        raise AssertionError(\"One of the expansions of the query {} is empty\".format(qid))\n",
    "\n",
    "    # check the tags\n",
    "    if any(tag is None\n",
    "           for and_query in candidates\n",
    "           for synset in and_query\n",
    "           for syn, tag in synset):\n",
    "        raise AssertionError(\"One of the tags of the query {} is None\".format(qid))\n",
    "\n",
    "    # check the base query\n",
    "    if len(base_query[0]) == 0:\n",
    "        raise AssertionError(\"The query {} contains an empty base query\".format(qid))\n",
    "\n",
    "    # check the shape of the base query and of the expansions\n",
    "    if len(candidates) != len(base_query) or any(len(candidates[i]) != len(base_query[i]) for i in xrange(len(candidates))):\n",
    "        raise AssertionError(\"The query {} has two different shapes for the base_query and candidates\".format(qid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"Number of queries\", len(qid_to_candidates)\n",
    "print \"Number of queries with zero expansions\", sum(1 for qid, candidates in qid_to_candidates.iteritems()\n",
    "                                                 if all(len(synset) <= 1 for and_query in candidates for synset in and_query))\n",
    "print \"Number of queries with improvements\", len(queries_with_recall_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ COLLECTION STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "collection_stats = PyCollectionStatsRestricted.load(cfg.thesaurus_dir + \"collection_stats_restricted.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "collection_stats_segment_to_segment_id = dict(\n",
    "    (segment, segment_id)\n",
    "    for segment_id, segment in cPickle.load(open(cfg.thesaurus_dir + \"collection_stats_restricted_segmentid_to_segment.pickle\", \"rb\")).iteritems()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"NumTerms: {: >8}\".format(collection_stats.get_num_terms())\n",
    "print \"CoOcc2:   {: >8}\".format(collection_stats.get_num_term_pairs())\n",
    "print \"CoOcc3:   {: >8}\".format(collection_stats.get_num_term_triples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES SUPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def identity(np_array):\n",
    "    return np_array\n",
    "\n",
    "def normalize_range(np_array):\n",
    "    min_array = np.nanmin(np_array, axis=0)\n",
    "    divisor = np.nanmax(np_array, axis=0) - min_array\n",
    "    divisor += (divisor == 0.0) * 1.0\n",
    "    return (np_array - min_array) / divisor\n",
    "\n",
    "def normalize_max(np_array):\n",
    "    #assert np_array.min() >= 0\n",
    "    divisor = np.nanmax(np_array, axis=0)\n",
    "    divisor += (divisor == 0.0) * 1.0\n",
    "    return np_array / divisor\n",
    "\n",
    "def normalize_rank(np_array):\n",
    "    np_array = np_array.T\n",
    "    ranks = np.empty(np_array.shape)\n",
    "    for r, row in enumerate(np_array):\n",
    "        ranks[r] = rankdata(row, method=\"min\")\n",
    "    return ranks.T\n",
    "\n",
    "def normalize_rank_descending(np_array):\n",
    "    return normalize_rank(-np_array)\n",
    "\n",
    "def standardize(np_array):\n",
    "    #assert np_array.min() >= 0\n",
    "    divisor = np.nanstd(np_array, axis=0)\n",
    "    divisor += (divisor == 0.0) * 1.0\n",
    "    return (np_array - np.nanmean(np_array, axis=0)) / divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tag_to_pos = dict(\n",
    "    (tag, i)\n",
    "    for i, tag in enumerate(sorted(set(\n",
    "        tag\n",
    "        for qid, exp_repr in qid_to_candidates.iteritems()\n",
    "        for and_query in exp_repr\n",
    "        for synset in and_query\n",
    "        for term, tags in synset  # exclude the first term since it is the source and has not tags\n",
    "        for tag in tags\n",
    "    )))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scoring_featurizer = fe.FeatureComposer([\n",
    "    fe.FeaturizerTextual(\n",
    "        feature_name_prefix=\"TEXT \"\n",
    "    ),\n",
    "    fe.FeaturizerTags(\n",
    "        tag_to_pos.keys(),\n",
    "        feature_name_prefix=\"TAG \"\n",
    "    ),\n",
    "    fe.FeatureNormalizer(\n",
    "        featurizer=fe.FeaturizerQueryPerformancePredictors(collection_stats, collection_stats_segment_to_segment_id),\n",
    "        normalization_name_function_list=[\n",
    "            (\"\", identity),\n",
    "            (\"NM \", normalize_max),\n",
    "            (\"NR \", normalize_range),\n",
    "        ],\n",
    "        feature_name_prefix=\"QPP \"\n",
    "    ),\n",
    "    fe.FeatureNormalizer(\n",
    "        featurizer=fe.FeaturizerSigIR08extended(collection_stats, collection_stats_segment_to_segment_id),\n",
    "        normalization_name_function_list=[\n",
    "            (\"\", identity),\n",
    "            (\"NM \", normalize_max),\n",
    "            (\"NR \", normalize_range),\n",
    "        ],\n",
    "        feature_name_prefix=\"SIGIRV2 \"\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING SUPPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class XGBModel(Model):\n",
    "    def __init__(self, model):\n",
    "        assert isinstance(model, xgboost.Booster)\n",
    "        self._model = model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ntree_limit = self._model.attributes().get(\"best_iteration\", 0)\n",
    "        return self._model.predict(xgboost.DMatrix(X), ntree_limit=ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BinaryModel(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class XGBBinaryClassifier(BinaryModel):\n",
    "    def __init__(self, model, threshold):\n",
    "        assert isinstance(model, xgboost.Booster)\n",
    "        self._model = model\n",
    "        self._threshold = threshold\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ntree_limit = self._model.attributes().get(\"best_iteration\", 0)\n",
    "        y = self._model.predict(xgboost.DMatrix(X), ntree_limit=ntree_limit)\n",
    "\n",
    "        return y >= self._threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_training_set(\n",
    "    idx_cache,\n",
    "    qid_list,\n",
    "    perf_to_metric,\n",
    "    features_featurizer,\n",
    "    sequential_greedy_selection=True,\n",
    "    scoring_model=None\n",
    "):\n",
    "    assert isinstance(sequential_greedy_selection, bool)\n",
    "    assert scoring_model is None or isinstance(scoring_model, Model)\n",
    "\n",
    "    global qid_to_base_query, qid_to_candidates\n",
    "\n",
    "    with idx_cache.cursor() as cursor:\n",
    "        dataset = dict()\n",
    "        queries = dict()\n",
    "\n",
    "        for qid in pb.iter_progress(qid_list):\n",
    "            dataset[qid] = []\n",
    "            queries[qid] = []\n",
    "\n",
    "            num_positive_documents = len(qid_to_docid_list[qid])\n",
    "            if num_positive_documents <= 0:\n",
    "                continue\n",
    "\n",
    "            # no improvements can be done, hence this query can be discarded from the training set\n",
    "            base_num_matches = get_num_match_query(qid, qid_to_base_query[qid])\n",
    "            if base_num_matches == num_positive_documents:\n",
    "                continue\n",
    "\n",
    "            curr_repr = copy.deepcopy(qid_to_base_query[qid])\n",
    "            exp_repr = copy.deepcopy(qid_to_candidates[qid])\n",
    "            num_candidates = qid_to_num_candidates[qid]\n",
    "\n",
    "            doc_id_list = qid_to_docid_list[qid]\n",
    "            w2o = qid_to_word_to_occurrence_set[qid]\n",
    "\n",
    "            base_metric = perf_to_metric(cursor.get_performance(\n",
    "                curr_repr,\n",
    "                doc_id_list,\n",
    "                qid,\n",
    "                True\n",
    "            ))\n",
    "\n",
    "            X_list = []\n",
    "            y_list = []\n",
    "\n",
    "            while num_candidates > 0:\n",
    "                X = features_featurizer.transform(curr_repr, exp_repr, num_candidates)\n",
    "                y = np.zeros(num_candidates)\n",
    "                if scoring_model is not None:\n",
    "                    y_scoring = scoring_model.predict(X)\n",
    "                    X = np.column_stack([X, y_scoring])\n",
    "\n",
    "                for abs_pos, and_pos, syn_pos, term_pos, term_tags in query_terms_iterator(exp_repr):\n",
    "                    curr_repr[and_pos][syn_pos].append(term_tags)\n",
    "                    if len(w2o[term_tags[0]] - w2o[curr_repr[and_pos][syn_pos][0][0]]) == 0:\n",
    "                        y[abs_pos] = 0\n",
    "                    elif not(base_num_matches < get_num_match_query(qid, curr_repr) >= 2):\n",
    "                        y[abs_pos] = 0\n",
    "                    else:\n",
    "                        y[abs_pos] = max(\n",
    "                            0.0,\n",
    "                            perf_to_metric(cursor.get_performance(\n",
    "                                curr_repr,\n",
    "                                doc_id_list,\n",
    "                                qid,\n",
    "                                True\n",
    "                            )) - base_metric\n",
    "                        )\n",
    "                    curr_repr[and_pos][syn_pos].pop()\n",
    "\n",
    "                dataset[qid].append((X,y))\n",
    "\n",
    "                # get the best term according to the oracle or according to the scoring model\n",
    "                if scoring_model is None:\n",
    "                    best_abs_pos = np.argmax(y)\n",
    "                else:\n",
    "                    best_abs_pos = np.argmax(y_scoring)\n",
    "\n",
    "                # get the score and the position of the best term\n",
    "                best_score = y[best_abs_pos]\n",
    "                best_tpl = None\n",
    "                for tpl in query_terms_iterator(exp_repr, only_positions=[best_abs_pos]):\n",
    "                    best_tpl = tpl\n",
    "                assert best_tpl is not None\n",
    "\n",
    "                # when to stop the sequential_greedy_selection\n",
    "                if best_score <= 0:\n",
    "                    break\n",
    "\n",
    "                # update the current representation\n",
    "                abs_pos, and_pos, syn_pos, term_pos, term_tags = best_tpl\n",
    "                curr_repr[and_pos][syn_pos].append(term_tags)\n",
    "                exp_repr[and_pos][syn_pos].pop(term_pos)\n",
    "                num_candidates -= 1\n",
    "\n",
    "                base_num_matches = get_num_match_query(qid, curr_repr)\n",
    "                base_metric = y[abs_pos] + base_metric\n",
    "\n",
    "                queries[qid].append([[[(term_tags[0],) for term_tags in synset] for synset in and_query] for and_query in curr_repr])\n",
    "\n",
    "                if not sequential_greedy_selection:\n",
    "                    break\n",
    "\n",
    "    return dataset, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dump_training_set(filename, dataset, queries):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        cPickle.dump(queries, outfile, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        for qid, Xy_list in pb.iteritems_progress(dataset):\n",
    "            cPickle.dump((qid, len(Xy_list)), outfile, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "            for X, y in Xy_list:\n",
    "                np.save(outfile, X)\n",
    "                np.save(outfile, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_dataset_set(filename):\n",
    "    with open(filename) as infile:\n",
    "        oracle = cPickle.load(infile)\n",
    "        dataset = dict()\n",
    "        for i in pb.iter_progress(xrange(len(oracle)), size=len(oracle)):\n",
    "            qid, num_Xy = cPickle.load(infile)\n",
    "            dataset[qid] = []\n",
    "            for j in xrange(num_Xy):\n",
    "                dataset[qid].append((np.load(infile), np.load(infile)))\n",
    "    return dataset, oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "full_qid_list = sorted(queries_with_recall_improvement)\n",
    "\n",
    "random.seed(seed)\n",
    "random.shuffle(full_qid_list)\n",
    "\n",
    "c1 = len(full_qid_list) * 70 / 100\n",
    "c2 = len(full_qid_list) * 85 / 100\n",
    "\n",
    "train_qid_list = full_qid_list[:c1]\n",
    "valid_qid_list = full_qid_list[c1:c2]\n",
    "test_qid_list = full_qid_list[c2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dataset_iterator(raw_dataset, qid_list, remove_last_step=True, only_first_step=False):\n",
    "    assert all(qid in raw_dataset for qid in qid_list)\n",
    "    for qid in qid_list:\n",
    "        query_dataset = raw_dataset[qid]\n",
    "        max_i = len(query_dataset)\n",
    "        if remove_last_step:\n",
    "            max_i -= 1\n",
    "        if only_first_step:\n",
    "            max_i = min(1, max_i)\n",
    "        for i in xrange(max_i):\n",
    "            yield qid, i, query_dataset[i][0], query_dataset[i][1]  # which is qid, step, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_top_k_arg(k, vec):\n",
    "    return vec.argsort()[-k:][::-1]  # faster\n",
    "    #return heapq.nlargest(k, np.arange(vec.size), key=(lambda p: vec[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_feval_gain_at_k(k, xgmatrix_to_groups):\n",
    "    assert isinstance(k, int) and k > 0\n",
    "    assert isinstance(xgmatrix_to_groups, dict)\n",
    "    assert all(isinstance(key, xgboost.DMatrix) and isinstance(value, np.ndarray) for key, value in xgmatrix_to_groups.iteritems())\n",
    "\n",
    "    metric_name = \"gain@{}\".format(k)\n",
    "    def feval_gain_at_k(preds, dtrain):\n",
    "        groups = xgmatrix_to_groups.get(dtrain, None)\n",
    "        assert groups is not None\n",
    "        labels = dtrain.get_label()\n",
    "\n",
    "        metric_sum = 0.0\n",
    "\n",
    "        l, r = 0, 0\n",
    "        for g in groups:\n",
    "            r = l + g\n",
    "            labels_bests = get_top_k_arg(k, labels[l:r])\n",
    "            pred_bests = get_top_k_arg(k, preds[l:r])\n",
    "            metric_sum += 1.0 * labels[l:r][pred_bests].sum() / (labels[l:r][labels_bests].sum() or 1.0)\n",
    "            l = r\n",
    "\n",
    "        return (metric_name, float(metric_sum) / len(groups))\n",
    "\n",
    "    return feval_gain_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_scoring_training(raw_dataset, qid_list, sequential_greedy_selection, transform_fun=None):\n",
    "    assert hasattr(qid_list, \"__iter__\") and all(qid in raw_dataset for qid in qid_list)\n",
    "    assert isinstance(sequential_greedy_selection, bool)\n",
    "    assert transform_fun is None or hasattr(transform_fun, \"__call__\")\n",
    "\n",
    "    kwargs = {\n",
    "        \"raw_dataset\": raw_dataset,\n",
    "        \"qid_list\": qid_list,\n",
    "        \"remove_last_step\": True,\n",
    "        \"only_first_step\": (not sequential_greedy_selection)\n",
    "    }\n",
    "\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    new_weights = []\n",
    "    new_groups = []\n",
    "    for qid, step, X, y in pb.iter_progress(dataset_iterator(**kwargs)):\n",
    "        if transform_fun:\n",
    "            X, y, w = transform_fun(X, y)\n",
    "        else:\n",
    "            y = np.clip(y, 0, 1)\n",
    "            w = np.ones(y.shape)\n",
    "\n",
    "        new_X.append(X)\n",
    "        new_y.append(y)\n",
    "        new_weights.append(w)\n",
    "        new_groups.append(X.shape[0])\n",
    "    \n",
    "    new_X = np.concatenate(new_X, axis=0)\n",
    "    new_y = np.concatenate(new_y)\n",
    "    new_weights = np.concatenate(new_weights)\n",
    "\n",
    "    xgmatrix = xgboost.DMatrix(\n",
    "        data=new_X,\n",
    "        label=new_y,\n",
    "        weight=new_weights,\n",
    "    )\n",
    "    xgmatrix.set_group(new_groups)\n",
    "    return xgmatrix, new_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE - StaticRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recall_dataset, recall_oracle = build_training_set(\n",
    "    idx_cache,\n",
    "    queries_with_recall_improvement,\n",
    "    perf_to_recall,\n",
    "    scoring_featurizer,\n",
    "    sequential_greedy_selection=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dump_training_set(cfg.tmp_dir + \"{}_dataset.pickle\".format(\"recall\"), recall_dataset, recall_oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "partition_to_xgmatrix = dict()\n",
    "xgmatrix_to_groups = dict()\n",
    "\n",
    "for what, qid_list in [(\"train\", train_qid_list), (\"valid\", valid_qid_list)]:\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    new_groups = []\n",
    "    for qid in pb.iter_progress(qid_list, labeling_fun={\"prefix\": what}):\n",
    "        y = recall_dataset[qid][0][1]\n",
    "        if y.max <= 0:\n",
    "            continue\n",
    "        new_X.append(recall_dataset[qid][0][0])\n",
    "        new_y.append(10.0 * np.clip(y, 0, 1))\n",
    "        new_groups.append(y.size)\n",
    "\n",
    "    xgmatrix = xgboost.DMatrix(\n",
    "        np.concatenate(new_X, axis=0),\n",
    "        label=np.concatenate(new_y, axis=0)\n",
    "    )\n",
    "    xgmatrix.set_group(new_groups)\n",
    "    partition_to_xgmatrix[what] = xgmatrix\n",
    "    xgmatrix_to_groups[xgmatrix] = np.array(new_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "baseline_scoring_model = xgboost.train(\n",
    "    params={\n",
    "        'objective': 'rank:pairwise',\n",
    "        'eval_metric': 'ndcg@1',\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "        'silent': 0,\n",
    "    },\n",
    "    num_boost_round=300,\n",
    "    dtrain=partition_to_xgmatrix[\"train\"],\n",
    "    evals=[(partition_to_xgmatrix[\"train\"], \"training\"), (partition_to_xgmatrix[\"valid\"], 'validation')],\n",
    "    early_stopping_rounds=20,\n",
    "    feval=get_feval_gain_at_k(5, xgmatrix_to_groups),\n",
    "    maximize=True\n",
    ")\n",
    "# [210]\ttraining-ndcg@1:0.741305\tvalidation-ndcg@1:0.737976\ttraining-gain@5:0.652657\tvalidation-gain@5:0.620137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_scoring_model.save_model(cfg.tmp_dir + \"scoring_recall_static.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del partition_to_xgmatrix, xgmatrix_to_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del recall_dataset, recall_oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATIC VS GREEDY EET200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eet200_dataset, eet200_oracle = build_training_set(\n",
    "    idx_cache,\n",
    "    queries_with_recall_improvement,\n",
    "    #get_perf_to_my_eet_function(200.0, 1.0),\n",
    "    perf_to_recall,\n",
    "    scoring_featurizer,\n",
    "    sequential_greedy_selection=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dump_training_set(cfg.tmp_dir + \"{}_dataset.pickle\".format(\"eet200\"), eet200_dataset, eet200_oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "partition_to_xgmatrix = dict()\n",
    "xgmatrix_to_groups = dict()\n",
    "for what, qid_list in [(\"train\", train_qid_list), (\"valid\", valid_qid_list)]:\n",
    "    for greedy in [True, False]:\n",
    "        name = \"{} {}\".format(what, \"greedy\" if greedy else \"static\")\n",
    "        xgmatrix, groups = get_scoring_training(eet200_dataset, qid_list, sequential_greedy_selection=greedy)\n",
    "        partition_to_xgmatrix[name] = xgmatrix\n",
    "        xgmatrix_to_groups[xgmatrix] = np.array(groups)\n",
    "        del xgmatrix, groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scoring_model_static = xgboost.train(\n",
    "    params={\n",
    "        'objective': 'rank:pairwise',\n",
    "        'eval_metric': 'ndcg@5',\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "        'silent': 0\n",
    "    },\n",
    "    num_boost_round=300,\n",
    "    dtrain=partition_to_xgmatrix[\"train static\"],\n",
    "    evals=[(partition_to_xgmatrix[\"train static\"], \"training\"), (partition_to_xgmatrix[\"valid static\"], 'validation')],\n",
    "    early_stopping_rounds=20,\n",
    "    feval=get_feval_gain_at_k(5, xgmatrix_to_groups),\n",
    "    maximize=True\n",
    ")\n",
    "# [252]\ttraining-ndcg@5:0.999804\tvalidation-ndcg@5:0.999803\ttraining-gain@5:0.673998\tvalidation-gain@5:0.637472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time scoring_model_static.save_model(cfg.tmp_dir + \"scoring_eet200_static.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GREEDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scoring_model_greedy = xgboost.train(\n",
    "    params={\n",
    "        'objective': 'rank:pairwise',\n",
    "        'eval_metric': 'ndcg@1',\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "        'silent': 0,\n",
    "    },\n",
    "    num_boost_round=300,\n",
    "    dtrain=partition_to_xgmatrix[\"train greedy\"],\n",
    "    evals=[(partition_to_xgmatrix[\"train greedy\"], \"training\"), (partition_to_xgmatrix[\"valid greedy\"], 'validation')],\n",
    "    early_stopping_rounds=20,\n",
    "    feval=get_feval_gain_at_k(1, xgmatrix_to_groups),\n",
    "    maximize=True\n",
    ")\n",
    "# 0.469493 in 182\n",
    "# [171]\ttraining-ndcg@1:0.999741\tvalidation-ndcg@1:0.999695\ttraining-gain@1:0.5059\tvalidation-gain@1:0.481192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time scoring_model_greedy.save_model(cfg.tmp_dir + \"scoring_eet200_greedy.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del partition_to_xgmatrix, xgmatrix_to_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scoring_model_static = xgboost.Booster()\n",
    "scoring_model_static.load_model(cfg.tmp_dir + \"scoring_eet200_static.model\")\n",
    "print scoring_model_static.attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scoring_model_greedy = xgboost.Booster()\n",
    "scoring_model_greedy.load_model(cfg.tmp_dir + \"scoring_eet200_greedy.model\")\n",
    "print scoring_model_greedy.attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_dataset_static, raw_queries_static = build_training_set(\n",
    "    idx_cache,\n",
    "    full_qid_list,\n",
    "    get_perf_to_my_eet_function(200.0, 1.0),\n",
    "    scoring_featurizer,\n",
    "    sequential_greedy_selection=False,\n",
    "    scoring_model=XGBModel(scoring_model_static)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dump_training_set(cfg.tmp_dir + \"{}_pruning_static_dataset.pickle\".format(\"eet200\"), raw_dataset_static, raw_queries_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_dataset_greedy, raw_queries_greedy = build_training_set(\n",
    "    idx_cache,\n",
    "    full_qid_list,\n",
    "    get_perf_to_my_eet_function(200.0, 1.0),\n",
    "    scoring_featurizer,\n",
    "    sequential_greedy_selection=True,\n",
    "    scoring_model=XGBModel(scoring_model_greedy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dump_training_set(cfg.tmp_dir + \"{}_pruning_greedy_dataset.pickle\".format(\"eet200\"), raw_dataset_greedy, raw_queries_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print len(idx_cache)\n",
    "idx_cache.dump(idx_cache_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from numpy cimport ndarray\n",
    "\n",
    "def find_threshold(\n",
    "    ndarray[np.float32_t, ndim=1] y_true,\n",
    "    ndarray[np.float32_t, ndim=1] y_pred\n",
    "):\n",
    "    cdef size_t best_p = 0\n",
    "    cdef size_t score = (y_true > 0).sum()\n",
    "    cdef size_t best_score = score\n",
    "    cdef size_t p\n",
    "\n",
    "    for p in np.argsort(y_pred):\n",
    "        if y_true[p] > 0.0:\n",
    "            score -= 1\n",
    "        else:\n",
    "            score += 1\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_p = p\n",
    "    \n",
    "    assert score == (y_true <= 0).sum()\n",
    "\n",
    "    return y_pred[best_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_pruning_training(raw_dataset, qid_list, greedy):\n",
    "    assert hasattr(qid_list, \"__iter__\") and all(qid in raw_dataset for qid in qid_list)\n",
    "    assert isinstance(greedy, bool)\n",
    "\n",
    "    kwargs = {\n",
    "        \"raw_dataset\": raw_dataset,\n",
    "        \"qid_list\": qid_list,\n",
    "        \"remove_last_step\": False,\n",
    "        \"only_first_step\": (not greedy)\n",
    "    }\n",
    "\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    new_weights = []\n",
    "    new_groups = []\n",
    "    for qid, step, X, y in pb.iter_progress(dataset_iterator(**kwargs)):\n",
    "        best_positions = get_top_k_arg(1 if greedy else 5, X[:,-1])\n",
    "        num_steps_qid = len(raw_dataset[qid])\n",
    "\n",
    "        X = X[best_positions]\n",
    "        y = y[best_positions]\n",
    "        w = np.full(len(best_positions), 1)\n",
    "\n",
    "        new_X.append(X)\n",
    "        new_y.append(y)\n",
    "        new_weights.append(w)\n",
    "        new_groups.append(X.shape[0])\n",
    "\n",
    "    xgmatrix = xgboost.DMatrix(\n",
    "        data=np.concatenate(new_X, axis=0),\n",
    "        label=np.concatenate(new_y),\n",
    "        weight=np.concatenate(new_weights),\n",
    "    )\n",
    "    xgmatrix.set_group(new_groups)\n",
    "    return xgmatrix, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "partition_to_xgmatrix = dict()\n",
    "xgmatrix_to_groups = dict()\n",
    "for what, qid_list in [(\"train\", train_qid_list), (\"valid\", valid_qid_list)]:\n",
    "    for greedy in [True, False]:\n",
    "        name = \"{} {}\".format(what, \"greedy\" if greedy else \"static\")\n",
    "        xgmatrix, groups = get_pruning_training(raw_dataset_greedy if greedy else raw_dataset_static, qid_list, greedy=greedy)\n",
    "        partition_to_xgmatrix[name] = xgmatrix\n",
    "        xgmatrix_to_groups[xgmatrix] = np.array(groups)\n",
    "        del xgmatrix, groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pruning_model_static = xgboost.train(\n",
    "    params={\n",
    "        'objective': 'reg:linear',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "#        'base_score': 0.5,\n",
    "        'scale_pos_weight': 0.8,  # should be something like sum(negative cases) / sum(positive cases)\n",
    "        'silent': 0\n",
    "    },\n",
    "    num_boost_round=200,\n",
    "    dtrain=partition_to_xgmatrix[\"train static\"],\n",
    "    evals=[(partition_to_xgmatrix[\"train static\"], \"training\"), (partition_to_xgmatrix[\"valid static\"], 'validation')],\n",
    "    early_stopping_rounds=20,\n",
    "    maximize=False\n",
    ")\n",
    "# [145]\ttraining-rmse:0.063551\tvalidation-rmse:0.088251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time pruning_model_static.save_model(cfg.tmp_dir + \"pruning_{}_static.model\".format(\"eet_200\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_true = partition_to_xgmatrix[\"train static\"].get_label()\n",
    "y_pred = pruning_model_static.predict(\n",
    "    partition_to_xgmatrix[\"train static\"],\n",
    "    ntree_limit=pruning_model_static.attr(\"best_iteration\")\n",
    ")\n",
    "\n",
    "pruning_threshold_static = find_threshold(y_true, y_pred)\n",
    "\n",
    "1.0 * ((y_pred >= pruning_threshold_static) == (y_true > 0)).sum() / y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print pruning_threshold_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GREEDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pruning_model_greedy = xgboost.train(\n",
    "    params={\n",
    "        'objective': 'reg:linear',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.09,\n",
    "#        'base_score': 0.5,\n",
    "        'scale_pos_weight': 0.7,  # should be something like sum(negative cases) / sum(positive cases)\n",
    "        'silent': 0\n",
    "    },\n",
    "    num_boost_round=200,\n",
    "    dtrain=partition_to_xgmatrix[\"train greedy\"],\n",
    "    evals=[(partition_to_xgmatrix[\"train greedy\"], \"training\"), (partition_to_xgmatrix[\"valid greedy\"], 'validation')],\n",
    "    early_stopping_rounds=20,\n",
    "    maximize=False\n",
    ")\n",
    "# 0.086712 with 102\n",
    "# [112]\ttraining-rmse:0.061863\tvalidation-rmse:0.088963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time pruning_model_greedy.save_model(cfg.tmp_dir + \"pruning_{}_greedy.model\".format(\"eet_200\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_true = partition_to_xgmatrix[\"train greedy\"].get_label()\n",
    "y_pred = pruning_model_greedy.predict(\n",
    "    partition_to_xgmatrix[\"train greedy\"],\n",
    "    ntree_limit=pruning_model_greedy.attr(\"best_iteration\")\n",
    ")\n",
    "\n",
    "pruning_threshold_greedy = find_threshold(y_true, y_pred)\n",
    "\n",
    "1.0 * ((y_pred >= pruning_threshold_greedy) == (y_true > 0)).sum() / y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print pruning_threshold_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del partition_to_xgmatrix, xgmatrix_to_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del raw_dataset_static, raw_dataset_greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name_model_list = [\n",
    "    (\"S2_Recall\", baseline_scoring_model),\n",
    "    (\"S2_EET\", scoring_model_static),\n",
    "    (\"SGS_EET\", scoring_model_greedy),\n",
    "    (\"SGS+Pruning_EET\", pruning_model_static),\n",
    "    (\"SGS+Pruning_EET\", pruning_model_greedy),\n",
    "]\n",
    "\n",
    "features = list(scoring_featurizer.feature_names()) + [\"scoring\"]\n",
    "features_scores = []\n",
    "for name, model in name_model_list:\n",
    "    print name\n",
    "    scores = np.zeros(len(features))\n",
    "    for feature_name, score in model.get_fscore().iteritems():\n",
    "        scores[int(feature_name[1:])] = score\n",
    "    features_scores.append(scores)\n",
    "    del scores\n",
    "\n",
    "df = pd.DataFrame(data=np.array(features_scores).T, index=features, columns=[name for name, _ in name_model_list])\n",
    "df.sort_values(by=\"S2_Recall\", inplace=True, ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_model_static_greedy(\n",
    "    scoring_featurizer,\n",
    "    scoring_model,\n",
    "    pruning_model,\n",
    "    qid,\n",
    "    num_terms=1,\n",
    "    greedy=True,\n",
    "    compute_unpruned=True,\n",
    "):\n",
    "    assert isinstance(scoring_model, Model)\n",
    "    assert (pruning_model is None) or isinstance(pruning_model, BinaryModel)\n",
    "    assert int(num_terms) > 0\n",
    "\n",
    "    num_terms = int(num_terms)\n",
    "\n",
    "    result_list_pruned = []\n",
    "    result_list_un_pruned = []\n",
    "\n",
    "    exp_repr = qid_to_candidates[qid]\n",
    "    num_candidates = qid_to_num_candidates[qid]\n",
    "\n",
    "    if not greedy:\n",
    "        base_repr = qid_to_base_query[qid]\n",
    "\n",
    "        # save the STATIC repr\n",
    "        def _save(target_list, positions, y_pruning):\n",
    "            static_repr = base_repr\n",
    "            for i, abs_pos in enumerate(positions):\n",
    "                if y_pruning[i]:\n",
    "                    # clone the query representation\n",
    "                    static_repr = copy.deepcopy(static_repr)\n",
    "                    # find the term in the query\n",
    "                    best_tpl = None\n",
    "                    for tpl in query_terms_iterator(exp_repr, only_positions=[abs_pos]):\n",
    "                        best_tpl = tpl\n",
    "                    assert best_tpl is not None and abs_pos == best_tpl[0] \n",
    "                    # add the term inside the query representation\n",
    "                    abs_pos, and_pos, syn_pos, term_pos, term_tags = best_tpl\n",
    "                    static_repr[and_pos][syn_pos].append(term_tags)\n",
    "                target_list.append(static_repr)\n",
    "\n",
    "        X = scoring_featurizer.transform(base_repr, exp_repr, num_candidates)\n",
    "        y_scor = scoring_model.predict(X)\n",
    "\n",
    "        best_positions = y_scor.argsort()[-num_terms:][::-1]\n",
    "        if compute_unpruned:\n",
    "            _save(result_list_un_pruned, best_positions, np.ones(best_positions.size))\n",
    "\n",
    "        if pruning_model:\n",
    "            X = np.column_stack([X[best_positions], y_scor[best_positions]])\n",
    "            y_post = pruning_model.predict(X)\n",
    "\n",
    "            _save(result_list_pruned, best_positions, y_post)\n",
    "    else:\n",
    "        greedy_repr = copy.deepcopy(qid_to_base_query[qid])\n",
    "        exp_repr = copy.deepcopy(exp_repr)\n",
    "        step = 0\n",
    "        pruned = False\n",
    "\n",
    "        while num_candidates > 0:\n",
    "            X = scoring_featurizer.transform(greedy_repr, exp_repr, num_candidates)\n",
    "            y_scor = scoring_model.predict(X)\n",
    "            step += 1\n",
    "\n",
    "            # save the GREEDY repre\n",
    "            y_argmax = y_scor.argmax()\n",
    "            best_tpl = None\n",
    "            for best_tpl in query_terms_iterator(exp_repr, only_positions=[y_argmax]):\n",
    "                pass\n",
    "            assert best_tpl is not None\n",
    "            best_positions = [best_tpl[0]]\n",
    "\n",
    "            # pruning\n",
    "            if pruning_model:\n",
    "                X = np.column_stack([X[best_positions], y_scor[best_positions]])\n",
    "                y = pruning_model.predict(X)\n",
    "                if not y[0]:\n",
    "                    pruned = True\n",
    "\n",
    "            # update the status\n",
    "            abs_pos, and_pos, syn_pos, term_pos, term_tags = best_tpl\n",
    "            greedy_repr[and_pos][syn_pos].append(term_tags)\n",
    "            exp_repr[and_pos][syn_pos].pop(term_pos)\n",
    "\n",
    "            if not pruned:\n",
    "                result_list_pruned.append(copy.deepcopy(greedy_repr))\n",
    "            if compute_unpruned:\n",
    "                result_list_un_pruned.append(copy.deepcopy(greedy_repr))\n",
    "\n",
    "            if pruned and not compute_unpruned:\n",
    "                break\n",
    "\n",
    "            if step >= num_terms:\n",
    "                break\n",
    "            num_candidates -= 1\n",
    "\n",
    "    return (result_list_pruned, result_list_un_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "terms_range = [1,3,5]\n",
    "max_terms_range = max(terms_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "representations = collections.OrderedDict()\n",
    "representations[\"NoEXP\"] = dict(\n",
    "    (qid, qid_to_base_query[qid])\n",
    "    for qid in test_qid_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for num_terms in terms_range:\n",
    "    representations[\"S2 Recall [{}]\".format(num_terms)] = dict()\n",
    "    representations[\"S2 EET [{}]\".format(num_terms)] = dict()\n",
    "    representations[\"S2 + Pruning EET [{}]\".format(num_terms)] = dict()\n",
    "    representations[\"SGS EET [{}]\".format(num_terms)] = dict()\n",
    "    representations[\"SGS + Pruning EET [{}]\".format(num_terms)] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _get_repr(qid, repr_list, num_terms):\n",
    "    _pos = num_terms - 1\n",
    "    if num_terms == 0:\n",
    "        return qid_to_base_query[qid]\n",
    "    _len = len(repr_list)\n",
    "    if _pos < _len:\n",
    "        return repr_list[_pos]\n",
    "    if _len > 0:\n",
    "        return repr_list[-1]\n",
    "    return qid_to_base_query[qid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for num_terms in terms_range:\n",
    "    representations[\"Oracle SGS + Pruning EET [{}]\".format(num_terms)] = dict(\n",
    "        (qid, _get_repr(qid, eet200_oracle[qid], num_terms))\n",
    "        for qid in test_qid_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for qid in pb.iter_progress(test_qid_list):\n",
    "    pruned_reprs, unpruned_reprs = apply_model_static_greedy(\n",
    "        scoring_featurizer,\n",
    "        scoring_model=XGBModel(baseline_scoring_model),\n",
    "        pruning_model=None,\n",
    "        qid=qid,\n",
    "        num_terms=max_terms_range,\n",
    "        greedy=False\n",
    "    )\n",
    "    for num_terms in terms_range:\n",
    "        representations[\"S2 Recall [{}]\".format(num_terms)][qid] = _get_repr(qid, unpruned_reprs, num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for qid in pb.iter_progress(test_qid_list):\n",
    "    pruned_reprs, unpruned_reprs = apply_model_static_greedy(\n",
    "        scoring_featurizer,\n",
    "        scoring_model=XGBModel(scoring_model_static),\n",
    "        pruning_model=XGBBinaryClassifier(pruning_model_static, pruning_threshold_static),\n",
    "        qid=qid,\n",
    "        num_terms=max_terms_range,\n",
    "        greedy=False\n",
    "    )\n",
    "    for num_terms in terms_range:\n",
    "        representations[\"S2 EET [{}]\".format(num_terms)][qid] = _get_repr(qid, unpruned_reprs, num_terms)\n",
    "        representations[\"S2 + Pruning EET [{}]\".format(num_terms)][qid] = _get_repr(qid, pruned_reprs, num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for qid in pb.iter_progress(test_qid_list):\n",
    "    pruned_reprs, unpruned_reprs = apply_model_static_greedy(\n",
    "        scoring_featurizer,\n",
    "        scoring_model=XGBModel(scoring_model_greedy),\n",
    "        pruning_model=XGBBinaryClassifier(pruning_model_greedy, pruning_threshold_greedy),\n",
    "        qid=qid,\n",
    "        num_terms=max_terms_range,\n",
    "        greedy=True\n",
    "    )\n",
    "    for num_terms in terms_range:\n",
    "        representations[\"SGS EET [{}]\".format(num_terms)][qid] = _get_repr(qid, unpruned_reprs, num_terms)\n",
    "        representations[\"SGS + Pruning EET [{}]\".format(num_terms)][qid] = _get_repr(qid, pruned_reprs, num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "representations[\"Oracle SGS + Pruning EET [{}]\"] = dict(\n",
    "    (qid, _get_repr(qid, eet200_oracle[qid], 100))\n",
    "    for qid in test_qid_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "performances = collections.OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with idx_cache.cursor() as cursor:\n",
    "    for what, qid2repr in representations.iteritems():\n",
    "        if len(qid2repr) == 0:\n",
    "            continue\n",
    "        if what not in performances:\n",
    "            performances[what] = dict()\n",
    "\n",
    "        for qid, query_repr in pb.iteritems_progress(qid2repr, labeling_fun={\"prefix\": what}, every=10):\n",
    "#            if qid in performances[what]:\n",
    "#                continue\n",
    "            performances[what][qid] = cursor.get_performance(query_repr, qid_to_docid_list[qid], qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print len(idx_cache)\n",
    "idx_cache.dump(idx_cache_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHOW RESULTS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "terms_range = [1,3,5]#range(1, 3+1)\n",
    "max_terms_range = max(terms_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = []\n",
    "table = collections.OrderedDict((name, []) for name in performances)\n",
    "\n",
    "columns.append(\"Exe Time\")\n",
    "for name, qid2perf in performances.iteritems():\n",
    "    tmp = [qid2perf[qid].exe_time for qid in test_qid_list]\n",
    "    table[name].append(\"{: >6.1f} ± {: >6.1f}\".format(\n",
    "        np.average(tmp),\n",
    "        np.std(tmp)\n",
    "    ))\n",
    "\n",
    "columns.append(\"Recall\")\n",
    "for name, qid2perf in performances.iteritems():\n",
    "    tmp = [100.0*perf_to_recall(qid2perf[qid]) for qid in test_qid_list]\n",
    "    table[name].append(\"{: >4.2f}% ± {: >4.1f}\".format(\n",
    "        np.average(tmp),\n",
    "        np.std(tmp)\n",
    "    ))\n",
    "\n",
    "columns.append(\"EET(200,1.0)\")\n",
    "f = get_perf_to_my_eet_function(200.0, 1.0)\n",
    "for name, qid2perf in performances.iteritems():\n",
    "    tmp = [100.0*f(qid2perf[qid]) for qid in test_qid_list]\n",
    "    table[name].append(\"{: >4.2f}% ± {: >4.1f}\".format(\n",
    "        np.average(tmp),\n",
    "        np.std(tmp)\n",
    "    ))\n",
    "\n",
    "columns.append(\"#terms\")\n",
    "for name, qid2perf in performances.iteritems():\n",
    "    def query_to_num_exp(query):\n",
    "        return sum(1 for tpl in query_terms_iterator(query) if tpl[3] != 0)  # tpl[3] is the term_pos inside the synset\n",
    "    tmp = [query_to_num_exp(representations[name][qid]) for qid in test_qid_list]\n",
    "    table[name].append(\"{:.1f} ± {:.1f}\".format(\n",
    "        np.average(tmp),\n",
    "        np.std(tmp)\n",
    "    ))\n",
    "\n",
    "df = pd.DataFrame(table.values(), table.keys(), columns)\n",
    "df.loc[\n",
    "    [\"NoEXP\"] + [\n",
    "        template.format(num_terms)\n",
    "        for num_terms in terms_range\n",
    "        for template in [\n",
    "            \"S2 Recall [{}]\",\n",
    "            \"S2 EET [{}]\",\n",
    "            \"SGS EET [{}]\",\n",
    "            \"S2 + Pruning EET [{}]\",\n",
    "            \"SGS + Pruning EET [{}]\",\n",
    "            \"Oracle SGS + Pruning EET [{}]\"\n",
    "        ]\n",
    "    ]\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
