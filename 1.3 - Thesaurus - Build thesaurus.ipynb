{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREAMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cfg  # to include python_libs\n",
    "\n",
    "import codecs\n",
    "import cPickle as cPickle\n",
    "import gzip\n",
    "import os\n",
    "import pattern.en\n",
    "\n",
    "import progress_bar as pb\n",
    "from efficient_query_expansion.normalize_text import normalize_text, normalize_hyphens, get_stopword_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOPWORDS AND GOOD TERMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# available at \n",
    "assert os.path.isfile(cfg.raw_dir + \"frequent_terms.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time stopwords = get_stopword_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this set of terms represents an overestimation of the good terms.\n",
    "# We filled it with all terms having document frequency greater than 20\n",
    "%time good_unary_terms = set(line.strip() for line in gzip.open(cfg.raw_dir + \"frequent_terms.txt.gz\"))\n",
    "print len(good_unary_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT FOR THE EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# segments and viceversa\n",
    "segment_to_segment_id = dict()\n",
    "segment_id_to_segment = []\n",
    "\n",
    "# spots segments\n",
    "segment_id_to_segment_id_segment_sim_list = dict()\n",
    "\n",
    "# entity-related segments\n",
    "entity_id_to_tags_segment_id_list = []\n",
    "segment_id_to_entity_id_tags_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "def _filter_segment_support(segment, stopwords, good_unary_terms):\n",
    "    if not segment:\n",
    "        return False\n",
    "\n",
    "    # split the segment into words\n",
    "    segment_split = segment.split()\n",
    "    # consider only aliases with at most 6 words (i.e. 5 spaces) and for which there is at least one word that is not a stopword.\n",
    "    return 0 < len(segment_split) <= 6 and all(w in good_unary_terms for w in segment_split) and not all(w in stopwords for w in segment_split)\n",
    "\n",
    "def _filter_support(segment_iterator, stopwords, good_unary_terms):\n",
    "    if not isinstance(segment_iterator, set):\n",
    "        segment_iterator = set(segment_iterator)\n",
    "    return [segment for segment in segment_iterator if _filter_segment_support(segment, stopwords, good_unary_terms)]\n",
    "\n",
    "def _add_segment_support(segment, segment_to_segment_id, segment_id_to_segment):\n",
    "    segment_id = segment_to_segment_id.get(segment, None)\n",
    "    if segment_id is None:\n",
    "        # new segment\n",
    "        segment_id = segment_to_segment_id[segment] = len(segment_id_to_segment)\n",
    "        segment_id_to_segment.append(segment)\n",
    "        return (segment_id, True)\n",
    "    else:\n",
    "        # segment already in\n",
    "        return (segment_id, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_entity_aliases(alias_to_tags, entity_tags):\n",
    "    global entity_id_to_tags_segment_id_list\n",
    "    global segment_id_to_entity_id_tags_list\n",
    "\n",
    "    # parameters check\n",
    "    assert isinstance(alias_to_tags, (dict)) and all(isinstance(alias, str) and isinstance(tags, tuple) and all(isinstance(tag, str) for tag in tags) for alias, tags in alias_to_tags.iteritems())\n",
    "    assert isinstance(entity_tags, tuple) and all(isinstance(tag, str) for tag in entity_tags)\n",
    "\n",
    "    # filter some aliases with respect to the filter_segment_support function\n",
    "    alias_to_tags = dict(\n",
    "        (alias, tags)\n",
    "        for alias, tags in alias_to_tags.iteritems()\n",
    "        if _filter_segment_support(alias, stopwords, good_unary_terms)\n",
    "    )\n",
    "    num_aliases = len(alias_to_tags)\n",
    "\n",
    "#     if num_aliases <= 1:\n",
    "#         return\n",
    "    # the previous filter has been replaced by the following one to include multi-term entities even if they haven't syns.\n",
    "    # The reason is that the segmentation will put toghether terms that if expanded alone will have a different meaning\n",
    "    if num_aliases <= 1:\n",
    "        if num_aliases == 0:\n",
    "            return\n",
    "        if \" \" not in alias_to_tags.keys()[0]:\n",
    "            return\n",
    "\n",
    "    # fill the support structures above\n",
    "    entity_id = len(entity_id_to_tags_segment_id_list)\n",
    "    segment_id_list = []\n",
    "    for segment, tags in alias_to_tags.iteritems():\n",
    "        segment_id, is_segment_new = _add_segment_support(segment, segment_to_segment_id, segment_id_to_segment)\n",
    "        entry = (entity_id, tags)\n",
    "        if is_segment_new:\n",
    "            segment_id_to_entity_id_tags_list.append((entry,))\n",
    "        else:\n",
    "            segment_id_to_entity_id_tags_list[segment_id] += (entry,)\n",
    "\n",
    "        segment_id_list.append(segment_id)\n",
    "    entity_id_to_tags_segment_id_list.append((entity_tags, tuple(segment_id_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILL THE SUPPORT STRUCTURES WITH THE WIKIPEDIA ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _alias_tags_str_to_dict_entry(alias_tags_str):\n",
    "    p = alias_tags_str.find(\":\")\n",
    "    return (alias_tags_str[:p], tuple(alias_tags_str[p+1:].split(\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALIASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wikipedia_entity_tags = (\"WPEnt\", )\n",
    "\n",
    "with gzip.open(cfg.processed_dir + \"wikipedia.aliases.tsv.gz\", \"r\") as infile:\n",
    "    for line in pb.iter_progress(infile):\n",
    "        p = line.find(\"\\t\")\n",
    "        entity_id = line[:p]\n",
    "        alias_to_tags = dict(\n",
    "            _alias_tags_str_to_dict_entry(alias_tags)\n",
    "            for alias_tags in line[p+1:-1].split(\"\\t\")  # -1 because the last character is always the \\n\n",
    "        )\n",
    "\n",
    "        # add this entity\n",
    "        add_entity_aliases(alias_to_tags, wikipedia_entity_tags)\n",
    "# it should last 17min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILL THE SUPPORT STRUCTURES WITH THE WIKIDATA ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wikidata_entity_tags = (\"WDEnt\", )\n",
    "\n",
    "with gzip.open(cfg.processed_dir + \"wikidata.aliases.tsv.gz\", \"r\") as infile:\n",
    "    for line in pb.iter_progress(infile):\n",
    "        p = line.find(\"\\t\")\n",
    "        entity_id = line[:p]\n",
    "\n",
    "        # exclude wiki properties from this export\n",
    "        if entity_id[0] == 'P':\n",
    "            continue\n",
    "\n",
    "        alias_to_tags = dict(\n",
    "            _alias_tags_str_to_dict_entry(alias_tags)\n",
    "            for alias_tags in line[p+1:-1].split(\"\\t\")  # -1 because the last character is always the \\n\n",
    "        )\n",
    "\n",
    "        add_entity_aliases(alias_to_tags, wikidata_entity_tags)\n",
    "# it should last 48min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILL THE SUPPORT STRUCTURES WITH THE OPENOFFICE ENTRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert os.path.isfile(cfg.raw_dir + \"thesaurus_en_openoffice_v1.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# thesaurus-related segments\n",
    "segment_id_to_meaning_id_list = dict()\n",
    "meaning_id_to_pos_segment_id_list = []\n",
    "\n",
    "reader = gzip.open(cfg.raw_dir + \"thesaurus_en_openoffice_v1.txt.gz\", \"rb\")\n",
    "try:\n",
    "    # ignore the header line which contains the encoding\n",
    "    encoding = reader.readline().strip()\n",
    "    # adjust the encoding reader\n",
    "    if encoding != \"ASCII\":\n",
    "        reader = codecs.getreader(encoding)(reader)\n",
    "\n",
    "    for row in pb.iter_progress(reader):\n",
    "        row = row.strip()\n",
    "        if row == \"\":\n",
    "            continue\n",
    "\n",
    "        # implicit check that the row contains only two values\n",
    "        word_raw, num_meanings = row.split(\"|\")\n",
    "        # check if the row is not a meaning of another word\n",
    "        if word_raw.startswith(\"(\"):\n",
    "            raise Exception(\"Bad original word format\")\n",
    "\n",
    "        key_list = _filter_support(\n",
    "            [normalize_text(word_raw), normalize_hyphens(word_raw)],\n",
    "            stopwords,\n",
    "            good_unary_terms\n",
    "        )\n",
    "        meanings = []\n",
    "        # add the hyphen normalization in the synset, in such a way to expand one form in the other\n",
    "        if len(key_list) > 1:\n",
    "            meanings.append(\n",
    "                (\"Hyph\", key_list)\n",
    "            )\n",
    "\n",
    "        # implicit check if num_meanings is an integer\n",
    "        for i in xrange(int(num_meanings)):\n",
    "            synonyms_raw = reader.readline().strip().split(\"|\")\n",
    "\n",
    "            if synonyms_raw[0][0] != '(' or synonyms_raw[0][-1] != ')':\n",
    "                raise Exception(\"POS not recognized on a meaning line\")\n",
    "            pos = str(synonyms_raw[0][1:-1])\n",
    "            synonyms_raw = synonyms_raw[1:]\n",
    "            synonyms = set(map(normalize_text, synonyms_raw) + map(normalize_hyphens, synonyms_raw))\n",
    "            #if pos == 'noun':\n",
    "            #    synonyms |= set(normalize_text(pattern.en.pluralize(synonym, pos)) for synonym in synonyms)\n",
    "\n",
    "            synset = _filter_support(\n",
    "                synonyms,\n",
    "                stopwords,\n",
    "                good_unary_terms\n",
    "            )\n",
    "            if len(synset) > 0:\n",
    "                meanings.append(\n",
    "                    (pos, synset)\n",
    "                )\n",
    "\n",
    "        # export if there are meanings to expand and if the key_list is not empty\n",
    "        if len(meanings) == 0 or len(key_list) == 0:\n",
    "            continue\n",
    "\n",
    "        key_id_list = [\n",
    "            _add_segment_support(key, segment_to_segment_id, segment_id_to_segment)[0]\n",
    "            for key in key_list\n",
    "        ]\n",
    "        start_meaning_len = len(meaning_id_to_pos_segment_id_list)\n",
    "        for pos, synset in meanings:\n",
    "            meaning_id_to_pos_segment_id_list.append(\n",
    "                (pos, tuple(\n",
    "                    _add_segment_support(term, segment_to_segment_id, segment_id_to_segment)[0]\n",
    "                    for term in synset\n",
    "                ))\n",
    "            )\n",
    "        meaning_id_list = tuple(xrange(start_meaning_len, len(meaning_id_to_pos_segment_id_list)))\n",
    "        for key_id in key_id_list:\n",
    "            if key_id in segment_id_to_meaning_id_list:\n",
    "                segment_id_to_meaning_id_list[key_id] += meaning_id_list\n",
    "            else:\n",
    "                segment_id_to_meaning_id_list[key_id] = meaning_id_list\n",
    "finally:\n",
    "    reader.close()\n",
    "# it should last 45s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORT DICT AND ENTITY SUPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Segments recognized:     {: >8}\".format(len(segment_id_to_segment))\n",
    "print \"Entity wikipedia entries:{: >8}\".format(len(set(segment_id for tags, segment_id_list in entity_id_to_tags_segment_id_list if tags[0] == \"WPEnt\" for segment_id in segment_id_list)))\n",
    "print \"Entity wikidata entries: {: >8}\".format(len(set(segment_id for tags, segment_id_list in entity_id_to_tags_segment_id_list if tags[0] == \"WDEnt\" for segment_id in segment_id_list)))\n",
    "print \"Entity toal entries:     {: >8}\".format(len(segment_id_to_entity_id_tags_list))\n",
    "print \"Thesaurus entries:       {: >8}\".format(len(segment_id_to_meaning_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# compact the lists\n",
    "segment_id_to_segment = tuple(segment_id_to_segment)\n",
    "\n",
    "entity_id_to_tags_segment_id_list = tuple(entity_id_to_tags_segment_id_list)\n",
    "segment_id_to_entity_id_tags_list = tuple(segment_id_to_entity_id_tags_list)\n",
    "\n",
    "meaning_id_to_pos_segment_id_list = tuple(meaning_id_to_pos_segment_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(cfg.thesaurus_dir + \"expansion_support.pickle\", \"wb\") as outfile:\n",
    "    cPickle.dump({\n",
    "            'segment_id_to_segment': segment_id_to_segment,\n",
    "\n",
    "            'entity_id_to_tags_segment_id_list': entity_id_to_tags_segment_id_list,\n",
    "            'segment_id_to_entity_id_tags_list': segment_id_to_entity_id_tags_list,\n",
    "\n",
    "            'segment_id_to_meaning_id_list': segment_id_to_meaning_id_list,\n",
    "            'meaning_id_to_pos_segment_id_list': meaning_id_to_pos_segment_id_list,\n",
    "        }, outfile, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
