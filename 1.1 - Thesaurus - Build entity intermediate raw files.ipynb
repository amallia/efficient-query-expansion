{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREAMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cfg\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import parallel_stream as ps\n",
    "import progress_bar as pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ THE WIKIPEDIA DUMP TO EXTRACT TITLES AND BOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "import wikiextractor.WikiExtractor as WE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOMIZED PAGE PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_html_parser = HTMLParser()\n",
    "\n",
    "# procedure used to read the input copyied from WikiExtractor\n",
    "def pages_from(row_iterator):\n",
    "    \"\"\"\n",
    "    Scans input extracting pages.\n",
    "    :return: (page_id, rev_id, page_title, redirect_title, page_rows), where page is a list of lines.\n",
    "    \"\"\"\n",
    "    # we collect individual lines, since str.join() is significantly faster than concatenation\n",
    "\n",
    "    # page details\n",
    "    page_rows = []\n",
    "    page_id = None\n",
    "    page_title = None\n",
    "    rev_id = None\n",
    "    redirect_title = None\n",
    "\n",
    "    # support variables\n",
    "    last_page_id = None\n",
    "    inside_page = False\n",
    "    inside_text = False\n",
    "\n",
    "    # loop over the rows\n",
    "    for line in row_iterator:\n",
    "        if not isinstance(line, WE.text_type):\n",
    "            line = line.decode('utf-8')\n",
    "        # check if the line can contain a tag\n",
    "        if '<' not in line:  # faster than doing re.search()\n",
    "            if inside_text:\n",
    "                page_rows.append(line)\n",
    "            continue\n",
    "\n",
    "        # check if the line contain a tag\n",
    "        m = WE.tagRE.search(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        tag = m.group(2)\n",
    "\n",
    "        if tag == 'page':\n",
    "            if inside_page:\n",
    "                WE.logging.warning(\"pages_from: After page_id {} nested page tag found\".format(last_page_id))\n",
    "            page_rows = []\n",
    "            page_id = None\n",
    "            page_title = None\n",
    "            rev_id = None\n",
    "            redirect_title = None\n",
    "            inside_page = True\n",
    "\n",
    "        elif not inside_page:\n",
    "            WE.logging.warning(\"pages_from: After page_id {} found a tag out of the page. Line: {}\".format(last_page_id, line))\n",
    "            continue\n",
    "\n",
    "        elif tag == '/text':\n",
    "            if not inside_text:\n",
    "                WE.logging.warning(\"pages_from: After page_id {} the tag </text> has been found, but not <text>\".format(last_page_id))\n",
    "            if m.group(1):\n",
    "                page_rows.append(m.group(1))\n",
    "            inside_text = False\n",
    "\n",
    "        elif inside_text:\n",
    "            page_rows.append(line)\n",
    "\n",
    "        elif tag == 'text':\n",
    "            if m.lastindex == 3 and line[m.start(3)-2] == '/': # self closing: <text xml:space=\"preserve\" />\n",
    "                continue\n",
    "            page_rows.append(line[m.start(3):m.end(3)])\n",
    "            inside_text = (m.lastindex != 4)  # open-close\n",
    "\n",
    "        elif tag == 'id':\n",
    "            if page_id is None:\n",
    "                page_id = m.group(3)\n",
    "            elif rev_id is None:\n",
    "                rev_id = m.group(3)\n",
    "\n",
    "        elif tag == 'title':\n",
    "            if page_title is not None:\n",
    "                WE.logging.warning(\"pages_from: After page_id {} page_title was already set ({} => {})\".format(last_page_id, page_title, m.group(3)))\n",
    "            page_title = m.group(3)\n",
    "\n",
    "        elif tag == 'redirect':\n",
    "            l, r = line.find(\"\\\"\")+1, line.rfind(\"\\\"\")\n",
    "            if l < r:\n",
    "                if redirect_title is not None:\n",
    "                    WE.logging.warning(\"pages_from: After page_id {} redirect_title was already set ({} => {})\".format(last_page_id, redirect_title, line[l:r]))\n",
    "                redirect_title = _html_parser.unescape(line[l:r])\n",
    "\n",
    "        elif tag == '/page':\n",
    "            if page_id != last_page_id:\n",
    "                yield (redirect_title, page_id, page_rows, page_title, page_rows)\n",
    "                last_page_id = page_id\n",
    "            else:\n",
    "                WE.logging.warning(\"pages_from: After page_id {} there is another page with the same id (can it be inside?)\".format(last_page_id))\n",
    "            inside_page = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOMIZED EXTRACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyExtractor(WE.Extractor):\n",
    "    headings_re = re.compile(\"^\\s*==(.*?)==\\s*$\")\n",
    "    mention_bold_re = re.compile(\"\\[START_MENTION_B\\](.*?)\\[END_MENTION_B\\]\")\n",
    "    mention_bolditalic_re = re.compile(\"\\[START_MENTION_BI\\](.*?)\\[END_MENTION_BI\\]\")\n",
    "\n",
    "    def __init__(self, page_id, page_revid, page_title, page_rows):\n",
    "        new_page_rows = []\n",
    "        for line in page_rows:\n",
    "            line = line\n",
    "            if MyExtractor.headings_re.match(line):\n",
    "                break\n",
    "            new_page_rows.append(\n",
    "                WE.bold.sub(\n",
    "                    r'[START_MENTION_B]\\1[END_MENTION_B]',\n",
    "                    WE.bold_italic.sub(\n",
    "                        r'[START_MENTION_BI]\\1[END_MENTION_BI]',\n",
    "                        line\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        super(MyExtractor, self).__init__(page_id, page_revid, page_title, new_page_rows)\n",
    "\n",
    "    def write_output(self, out, text):\n",
    "        matches = set(\n",
    "            (\n",
    "                BeautifulSoup(match).getText() if \"<\" in match else match  # fix for nested markup inside templates\n",
    "            ).strip().replace(\"\\t\", \" \")\n",
    "            for line in text\n",
    "            for match in (MyExtractor.mention_bold_re.findall(line) + MyExtractor.mention_bolditalic_re.findall(line))\n",
    "        )\n",
    "        matches.discard(\"\")\n",
    "\n",
    "        out.update(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INIT THE LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(cfg.processed_dir + \"wikipedia_raw/\"):\n",
    "    os.mkdir(cfg.processed_dir + \"wikipedia_raw/\")\n",
    "\n",
    "if not os.path.isdir(cfg.processed_dir + \"wikidata_raw/\"):\n",
    "    os.mkdir(cfg.processed_dir + \"wikidata_raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_template_input = cfg.processed_dir + \"enwiki-latest-pages-articles_parts/part_{{}}_{}.xml.gz\".format(cfg.wiki_preprocessing_split_into)\n",
    "path_template_aliases = cfg.processed_dir + \"wikipedia_raw/aliases.part_{{}}_{}.tsv.gz\".format(cfg.wiki_preprocessing_split_into)\n",
    "path_template_redirects = cfg.processed_dir + \"wikipedia_raw/redirects.part_{{}}_{}.tsv.gz\".format(cfg.wiki_preprocessing_split_into)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# collect siteinfo to set the options (copyied from wikiextractor)\n",
    "for line in gzip.open(path_template_input.format(1), \"r\"):\n",
    "    # When an input file is .bz2 or .gz, line can be a bytes even in Python 3.\n",
    "    if not isinstance(line, WE.text_type):\n",
    "        line = line.decode('utf-8')\n",
    "    m = WE.tagRE.search(line)\n",
    "    if not m:\n",
    "        continue\n",
    "    tag = m.group(2)\n",
    "    if tag == 'base':\n",
    "        # discover urlbase from the xml dump file\n",
    "        # /mediawiki/siteinfo/base\n",
    "        base = m.group(3)\n",
    "        WE.options.urlbase = base[:base.rfind(\"/\")]\n",
    "    elif tag == 'namespace':\n",
    "        mk = WE.keyRE.search(line)\n",
    "        WE.options.knownNamespaces[m.group(3)] = mk.group(1) if mk else ''\n",
    "        if re.search('key=\"10\"', line):\n",
    "            WE.options.templateNamespace = m.group(3)\n",
    "            WE.options.templatePrefix = WE.options.templateNamespace + ':'\n",
    "        elif re.search('key=\"828\"', line):\n",
    "            WE.options.moduleNamespace = m.group(3)\n",
    "            WE.options.modulePrefix = WE.options.moduleNamespace + ':'\n",
    "    elif tag == '/siteinfo':\n",
    "        break\n",
    "\n",
    "# set the other options\n",
    "WE.options.expand_templates = True\n",
    "WE.options.filter_disambig_pages = True\n",
    "WE.options.toHTML = False\n",
    "# set the logger level to avoid warning (mainly due to template substitutions)\n",
    "WE.logging.basicConfig(filename=\"/tmp/wikipedia_processing.log\", filemode=\"w\", level=WE.logging.WARN, format=\"%(asctime)s|%(levelname)s|%(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD THE TEMPLATES USED TO EXPAND THE MACROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of a bug into wikiextractor we disable this useful option"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# load the templates. This operation must be done after the options have been set\n",
    "with gzip.open(cfg.processed_dir + \"enwiki-latest-pages-articles.templates.xml.gz\") as templatefile:\n",
    "    WE.load_templates(templatefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT RAW REDIRECTS, TITLES AND BOLDS INTO INTERMEDIATE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class Timer(object):\n",
    "    \"\"\"Timer class using ALARM signal.\"\"\"\n",
    "\n",
    "    class TimeoutException(Exception):\n",
    "        def __init__(self, message=\"TimeoutException\"):\n",
    "            super(Timer.TimeoutException, self).__init__(message)\n",
    "\n",
    "    def __init__(self, seconds):\n",
    "        assert isinstance(seconds, (int, long)) and seconds > 0\n",
    "        self._seconds = seconds\n",
    "\n",
    "    def __enter__(self):\n",
    "        # set alarm handler\n",
    "        signal.signal(signal.SIGALRM, self.raise_timeout_exception)\n",
    "        # set alarm\n",
    "        signal.alarm(self._seconds)\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        # disable alarm\n",
    "        signal.alarm(0)\n",
    "\n",
    "    def raise_timeout_exception(self, *args):\n",
    "        WE.logging.error(u\"TimeoutException raised\")\n",
    "        raise Timer.TimeoutException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _get_emitter(part):\n",
    "    def _emitter(outqueue):\n",
    "        with gzip.open(path_template_input.format(part), \"r\") as infile:\n",
    "            # skip the header on the first file\n",
    "            if part == 1:\n",
    "                for line in infile:\n",
    "                    if line == \"  </siteinfo>\\n\":\n",
    "                        break\n",
    "\n",
    "            # variables used by the next loop\n",
    "            buffer_rows = []\n",
    "            buffer_chars = 0\n",
    "            buffer_max_chars = 256 * 1024\n",
    "\n",
    "            for line in infile:\n",
    "                if line == \"  <page>\\n\":\n",
    "                    # put the buffer into the outqueue when it is big enough\n",
    "                    if buffer_chars >= buffer_max_chars:\n",
    "                        outqueue.put(buffer_rows)\n",
    "                        buffer_rows = []\n",
    "                        buffer_chars = 0\n",
    "\n",
    "                buffer_rows.append(line)\n",
    "                buffer_chars += len(line)\n",
    "\n",
    "            # put the remaining part into the outqueue\n",
    "            if buffer_chars > 0:\n",
    "                outqueue.put(buffer_rows)\n",
    "                buffer_rows = []\n",
    "                buffer_chars = 0\n",
    "    return _emitter\n",
    "\n",
    "def _worker(worker_id, inqueue, outqueue):\n",
    "    # support set used to accumulate the aliases\n",
    "    timer = Timer(seconds=60)\n",
    "    aliases_raw = set()\n",
    "\n",
    "    for buffer_rows in inqueue:\n",
    "        # pages_from goes over the portion of xml and yields page_data each time a page is recognized\n",
    "        for page_data in pages_from(buffer_rows):\n",
    "            (page_redirect_title, page_id, page_revid, page_title, page_rows) = page_data\n",
    "\n",
    "            try:\n",
    "                if page_redirect_title is None:  # aliases\n",
    "                    # reset the set\n",
    "                    aliases_raw.clear()\n",
    "\n",
    "                    # extract the useful informations using my extractor\n",
    "                    # use a timer to block the Extractor when it reaches a self-loop state (issue of the library)\n",
    "                    with timer:\n",
    "                        e = MyExtractor(page_id, page_revid, page_title, page_rows)\n",
    "                        e.extract(aliases_raw)\n",
    "                    aliases_raw.discard(page_title)\n",
    "\n",
    "                    # normalize the title\n",
    "                    #page_title = normalize_text(page_title)\n",
    "\n",
    "                    # normalize the aliases\n",
    "                    #aliases_raw.add(page_title)\n",
    "                    #aliases = normalize_aliases_raw(aliases_raw)\n",
    "                    #aliases.discard(page_title)\n",
    "\n",
    "                    op = 0\n",
    "                    #line = \"{}\\t{}\\t{}\\n\".format(page_id, page_title, \"\\t\".join(aliases))\n",
    "                    line = u\"{}\\t{}\\t{}\\n\".format(page_id, page_title, u\"\\t\".join(aliases_raw))\n",
    "\n",
    "                    # free memory\n",
    "                    del e\n",
    "                    #del aliases\n",
    "                else:  # redirects\n",
    "\n",
    "                    op = 1\n",
    "                    #line = \"{}\\t{}\\t{}\\n\".format(page_id, normalize_text(page_title), normalize_text(page_redirect_title))\n",
    "                    line = u\"{}\\t{}\\t{}\\n\".format(page_id, page_title, page_redirect_title)\n",
    "\n",
    "                outqueue.put((op, line.encode('utf-8')))\n",
    "                del line\n",
    "            except Timer.TimeoutException:\n",
    "                WE.logging.error(u\"TimeoutException in the worker at page_id '{}' (page_title '{}')\".format(page_id, page_title))\n",
    "            except Exception as e:\n",
    "                WE.logging.error(u\"Exception in the worker at page_id '{}' (page_title '{}'): '{}'\".format(page_id, page_title, str(e)))\n",
    "\n",
    "            # free memory\n",
    "            del page_rows\n",
    "            del page_data\n",
    "        del buffer_rows\n",
    "\n",
    "def _get_collector(part, pb, total_count):\n",
    "    def _collector(inqueue):\n",
    "        with gzip.open(path_template_aliases.format(part), \"w\") as aliases_file,\\\n",
    "        gzip.open(path_template_redirects.format(part), \"w\") as redirects_file:\n",
    "            internal_count = total_count  # fix: we need to do this assignment. See: https://stackoverflow.com/questions/7535857/why-doesnt-this-closure-modify-the-variable-in-the-enclosing-scope/7535919#7535919\n",
    "            # write on file the outputs\n",
    "            for op, line in inqueue:\n",
    "                (aliases_file if op == 0 else redirects_file).write(line)\n",
    "                pb.increase()\n",
    "                internal_count += 1\n",
    "\n",
    "                if internal_count % 50000 == 0:\n",
    "                    WE.logging.warn(\"Processed {} pages\".format(internal_count))\n",
    "            return internal_count\n",
    "    return _collector\n",
    "\n",
    "# start the parallel computation\n",
    "pb_files = pb.ProgressBar(size=cfg.wiki_preprocessing_split_into, labeling_fun={\"prefix\":\"Files\"})\n",
    "pb_collector = pb.ProgressBar(every=500)\n",
    "try:\n",
    "    # there are about 17M pages in the dump (17.146.932 <page> 17.152.607 </page> 17.150.956 </text>)\n",
    "    overall_count = 0\n",
    "    for part in xrange(1, cfg.wiki_preprocessing_split_into+1):\n",
    "        WE.logging.warn(\"Start to process part {} of {}\".format(part, cfg.wiki_preprocessing_split_into))\n",
    "\n",
    "        iter_count = ps.parallel_stream(\n",
    "            _get_emitter(part),\n",
    "            _worker,\n",
    "            _get_collector(part, pb_collector, overall_count),\n",
    "            emitter_output_chunk_size=1,\n",
    "            worker_output_chunk_size=100,\n",
    "            emitter_queue_size=100,\n",
    "            collector_queue_size=100,\n",
    "            fork_collector=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        WE.logging.warn(\"Processed {} pages in the current part. Total processed: {}\".format(iter_count - overall_count, iter_count))\n",
    "        overall_count = iter_count\n",
    "        pb_files.increase()\n",
    "    pb_files.stop(True)\n",
    "    pb_collector.stop(True)\n",
    "except:\n",
    "    pb_files.stop(False)\n",
    "    pb_collector.stop(False)\n",
    "    raise\n",
    "\n",
    "# it lasts 1h 14min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del WE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS WIKIDATA RAW FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.mediawiki.org/wiki/Wikibase/DataModel/JSON#Claims_and_Statements\n",
    "assert os.path.isfile(cfg.raw_dir + \"wikidata-all.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _emitter(outqueue):\n",
    "    with gzip.open(cfg.raw_dir + \"wikidata-all.json.gz\", \"r\") as infile:\n",
    "        # skil the first line that contains \n",
    "        infile.readline()\n",
    "\n",
    "        for line in infile:\n",
    "            outqueue.put(line)\n",
    "\n",
    "def _worker(worker_id, inqueue, outqueue):\n",
    "    aliases_raw = set()\n",
    "    for line in inqueue:\n",
    "        line = line.rstrip(\",\\n\")\n",
    "        if line == \"]\":\n",
    "            continue\n",
    "        entity = json.loads(line)\n",
    "\n",
    "        # get entity id\n",
    "        entity_id = entity['id'].encode(\"ascii\")\n",
    "\n",
    "        # get entity label\n",
    "        label = \"\"\n",
    "        if \"en\" in entity['labels']:\n",
    "            label = entity['labels'][\"en\"][\"value\"].replace(\"\\t\", \" \")\n",
    "            # I put the label into the set to avoid aliases which are duplicates\n",
    "            aliases_raw.add(label)\n",
    "\n",
    "        # get aliases\n",
    "        if \"en\" in entity[\"aliases\"]:\n",
    "            aliases_raw.update(alias_obj[\"value\"].replace(\"\\t\", \" \") for alias_obj in entity[\"aliases\"][\"en\"])\n",
    "\n",
    "        if len(aliases_raw) == 0:\n",
    "            continue\n",
    "\n",
    "        # discard the label from the aliases, because it is put a part\n",
    "        aliases_raw.discard(label)\n",
    "\n",
    "        line = u\"{}\\t{}\\t{}\\n\".format(entity_id, label, u\"\\t\".join(alias_raw for alias_raw in aliases_raw))\n",
    "\n",
    "        # send this aliases to the collector\n",
    "        outqueue.put(line.encode('utf-8'))\n",
    "\n",
    "        # reset the support set\n",
    "        aliases_raw.clear()\n",
    "\n",
    "def _collector(inqueue):\n",
    "    with gzip.open(cfg.processed_dir + \"wikidata_raw/aliases.tsv.gz\", \"w\") as outfile:\n",
    "        for line in pb.iter_progress(inqueue):\n",
    "            outfile.write(line)\n",
    "\n",
    "ps.parallel_stream(\n",
    "    _emitter,\n",
    "    _worker,\n",
    "    _collector,\n",
    "    emitter_output_chunk_size=100,\n",
    "    worker_output_chunk_size=100,\n",
    "    emitter_queue_size=100,\n",
    "    collector_queue_size=100,\n",
    "    fork_collector=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# it lasts 1hour 2min"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
