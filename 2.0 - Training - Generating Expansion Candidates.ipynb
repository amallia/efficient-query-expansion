{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cfg\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "import pattern.en\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import progress_bar as pb\n",
    "from documents_utils import doc_generator_from_file\n",
    "from normalize_text import normalize_text, get_stopword_set\n",
    "\n",
    "from pattern_matching.pattern_matcher import PyPatternMatcher, PyPatternMatches\n",
    "from pattern_matching.segmenter import PySegmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD ALL THE DOCUMENTS, QUERIES AND ASSOCIATIONS IN MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_id_to_query = cPickle.load(open(cfg.raw_dir + \"training/query_id_to_query.pickle\", \"rb\"))\n",
    "query_id_to_doc_id_list = cPickle.load(open(cfg.raw_dir + \"training/query_id_to_doc_id_list.pickle\", \"r\"))\n",
    "doc_id_to_raw_text = cPickle.load(open(cfg.raw_dir + \"training/doc_id_to_raw_text.pickle\", \"r\"))\n",
    "\n",
    "assert len(query_id_to_query) == len(query_id_to_doc_id_list)\n",
    "assert all(doc_id in doc_id_to_text for doc_id_list in query_id_to_doc_id_list.itervalues() for doc_id in doc_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "doc_id_to_text = dict((doc_id, normalize_text(raw_text)) for doc_id, raw_text in doc_id_to_raw_text.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# revert the query_id_to_doc_id_list associations\n",
    "doc_id_to_query_id_list = dict()\n",
    "for query_id, doc_id_list in query_id_to_doc_id_list.iteritems():\n",
    "    for doc_id in doc_id_list:\n",
    "        if doc_id in doc_id_to_query_id_list:\n",
    "            doc_id_to_query_id_list[doc_id].append(query_id)\n",
    "        else:\n",
    "            doc_id_to_query_id_list[doc_id] = [query_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "good_terms = set(line.strip() for line in gzip.open(cfg.raw_dir + \"frequent_terms.txt.gz\"))\n",
    "print len(good_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_good_expansion(expansion):\n",
    "    global good_terms\n",
    "\n",
    "    if \" \" in expansion:\n",
    "        return all((term in good_terms) for term in expansion.split())\n",
    "    else:\n",
    "        return expansion in good_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import pattern.en\n",
    "\n",
    "def term_to_lemma(term, pos):\n",
    "    if \" \" in term:\n",
    "        lemma = ' '.join(pattern.en.lemma(t) or t for t in term.split())\n",
    "    else:\n",
    "        lemma = pattern.en.lemma(term)\n",
    "    return str(lemma).strip()\n",
    "\n",
    "def term_to_plural(term, pos):\n",
    "    return str(pattern.en.pluralize(term, pos)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "def query_match(or_query, text):\n",
    "    return any(  # or level: at least one of the OR macro terms must match the text\n",
    "        all(  # and level: each synset must match the text\n",
    "            any(  # or level: at least one of the word in the synset must be in the text\n",
    "                (\" \" + word_and_tags[0] + \" \") in text\n",
    "                for word_and_tags in synset\n",
    "            ) for synset in and_query\n",
    "        ) for and_query in or_query\n",
    "        if len(and_query) > 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REWRITING STRATEGIES SUPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collection dependent term-statistics. This dictionary depends from the dataset\n",
    "term_to_df = cPickle.load(open(cfg.processed_dir + \"term_to_df.pickle\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time segments_thesaurus = frozenset(line[:-1] for line in open(cfg.thesaurus_dir + \"thesaurus.dict\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = get_stopword_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time expansion_support = cPickle.load(open(thesaurus_dir + \"expansion_support.pickle\", \"rb\"))\n",
    "\n",
    "%time expansion_support[\"segment_to_segment_id\"] = dict((segment, segment_id) for segment_id, segment in enumerate(expansion_support[\"segment_id_to_segment\"]))\n",
    "assert len(expansion_support[\"segment_to_segment_id\"]) == len(expansion_support[\"segment_id_to_segment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pos_to_lemma_to_segment_id_set = {'adj': {}, 'adv': {}, 'noun': {}, 'verb': {}}\n",
    "\n",
    "for segment_id, meaning_id_list in expansion_support['segment_id_to_meaning_id_list'].iteritems():\n",
    "    term = expansion_support['segment_id_to_segment'][segment_id]\n",
    "    # iterate over the possible meanings and take the pos tags\n",
    "    for pos in set(expansion_support['meaning_id_to_pos_segment_id_list'][meaning_id][0] for meaning_id in meaning_id_list):\n",
    "        if pos not in pos_to_lemma_to_segment_id_set:\n",
    "            continue\n",
    "        lemma = term_to_lemma(term, pos)\n",
    "\n",
    "        # update the dictionaries\n",
    "        if lemma in pos_to_lemma_to_segment_id_set[pos]:\n",
    "            pos_to_lemma_to_segment_id_set[pos][lemma].add(segment_id)\n",
    "        else:\n",
    "            pos_to_lemma_to_segment_id_set[pos][lemma] = set([segment_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "collapsed_segment_to_segment_id_list = dict()\n",
    "\n",
    "for segment_id in xrange(max(\n",
    "    len(expansion_support['segment_id_to_entity_id_tags_list']),\n",
    "    1\n",
    ")):\n",
    "    segment = expansion_support['segment_id_to_segment'][segment_id]\n",
    "    if \" \" in segment:\n",
    "        new_segment = segment.replace(\" \", \"\")\n",
    "        if new_segment in expansion_support['segment_to_segment_id']:\n",
    "            continue\n",
    "\n",
    "        if new_segment in collapsed_segment_to_segment_id_list:\n",
    "            collapsed_segment_to_segment_id_list[new_segment] += (segment_id,)\n",
    "        else:\n",
    "            collapsed_segment_to_segment_id_list[new_segment] = (segment_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "def group_or_terms(or_term):\n",
    "    term_to_tags = dict()\n",
    "    for term, tags in or_term:\n",
    "        if term not in term_to_tags:\n",
    "            term_to_tags[term] = tags\n",
    "        else:\n",
    "            term_to_tags[term] += tuple(tag for tag in tags if tag not in term_to_tags[term])\n",
    "\n",
    "    return [\n",
    "        (term, tags)\n",
    "        for term, tags in term_to_tags.iteritems()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_source_term(term):\n",
    "    return (term,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_expansions(term_tags_list, query_terms):\n",
    "    return [\n",
    "        term_tags\n",
    "        for term_tags in term_tags_list\n",
    "        if all(term_tags[0] != query_term for query_term in query_terms)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(query, query_segmenter=None):\n",
    "    # create a backup of the query\n",
    "    query_backup = query\n",
    "    # remove the stop words according to if they belong to some entity or not\n",
    "    query = filter((lambda x: x not in stopwords), (query_segmenter.segment(query) if query_segmenter else query.split()))\n",
    "\n",
    "    # ACK: if the query is composed only by stopwords use all the terms as query\n",
    "    if len(query) == 0:\n",
    "        query = query_backup\n",
    "    else:\n",
    "        # discard the previous segmentation\n",
    "        query = \" \".join(query)\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_thesaurus_expansions(term):\n",
    "    pos_set = pos_to_lemma_to_segment_id_set.keys()\n",
    "\n",
    "    # get the LEMMA for each possible pos tag\n",
    "    pos_to_lemma = dict(\n",
    "        (pos, term_to_lemma(term, pos))\n",
    "        for pos in pos_set\n",
    "    )\n",
    "    # filtering unlikely lemmas\n",
    "    if False:\n",
    "        for pos in pos_set:\n",
    "            if pos_to_lemma[pos] not in pos_to_lemma_to_segment_id_set[pos]:\n",
    "                del pos_to_lemma[pos]\n",
    "\n",
    "    # use the lemma only if this term doesn't appear in our segments\n",
    "    if False and term in expansion_support['segment_to_segment_id']:\n",
    "        segment_id = expansion_support['segment_to_segment_id'][term]\n",
    "        if segment_id in expansion_support['segment_id_to_meaning_id_list']:\n",
    "            meaning_pos_set = set(\n",
    "                expansion_support['meaning_id_to_pos_segment_id_list'][meaning_id][0]\n",
    "                for meaning_id in expansion_support['segment_id_to_meaning_id_list'][segment_id]\n",
    "            )\n",
    "        else:\n",
    "            meaning_pos_set = set()\n",
    "\n",
    "        segment_id_set = set([segment_id])\n",
    "        pos_to_normalized_segment_id_set = dict(\n",
    "            (pos, segment_id_set if pos in meaning_pos_set else set())\n",
    "            for pos in pos_set\n",
    "        )\n",
    "    else:\n",
    "        # find possible NORMALIZED versions of the lemma for each pos tag\n",
    "        pos_to_normalized_segment_id_set = dict(\n",
    "            #(pos, pos_to_lemma_to_segment_id_set[pos][lemma])\n",
    "            (pos, pos_to_lemma_to_segment_id_set[pos][lemma] if lemma in pos_to_lemma_to_segment_id_set[pos] else set())\n",
    "            for (pos, lemma) in pos_to_lemma.iteritems()\n",
    "        )\n",
    "\n",
    "    pos_to_normalized_term_set = dict(\n",
    "        (pos, set(expansion_support['segment_id_to_segment'][segment_id] for segment_id in normalized_segment_id_list))\n",
    "        for (pos, normalized_segment_id_list) in pos_to_normalized_segment_id_set.iteritems()\n",
    "    )\n",
    "\n",
    "    # get the SYNONYMS of each normalized version\n",
    "    pos_to_synset = dict()\n",
    "    for pos, normalized_segment_id_list in pos_to_normalized_segment_id_set.iteritems():\n",
    "        pos_to_synset[pos] = set(\n",
    "            expansion_support['segment_id_to_segment'][segment_id]\n",
    "            for normalized_segment_id in normalized_segment_id_list\n",
    "            for meaning_id in expansion_support['segment_id_to_meaning_id_list'][normalized_segment_id]\n",
    "                if normalized_segment_id in expansion_support['segment_id_to_meaning_id_list']\n",
    "                and pos == expansion_support['meaning_id_to_pos_segment_id_list'][meaning_id][0]\n",
    "            for segment_id in expansion_support['meaning_id_to_pos_segment_id_list'][meaning_id][1]\n",
    "                if (\" \" + expansion_support['segment_id_to_segment'][normalized_segment_id] + \" \") not in (\" \" + expansion_support['segment_id_to_segment'][segment_id] + \" \")  # discard synonyms that extend the starting term with additional terms\n",
    "        )\n",
    "\n",
    "    # get the PLURALS of the normalized terms and their synonyms (which should be in the singular form)\n",
    "    terms_to_pluralize = set()\n",
    "    if \"noun\" in pos_to_normalized_term_set:\n",
    "        terms_to_pluralize.update(pos_to_normalized_term_set[\"noun\"])\n",
    "    if \"noun\" in pos_to_synset:\n",
    "        terms_to_pluralize.update(pos_to_synset[\"noun\"])\n",
    "\n",
    "    noun_plurals = set(\n",
    "        term_to_plural(new_term, \"noun\")\n",
    "        for new_term in terms_to_pluralize\n",
    "    )\n",
    "\n",
    "    # put all togheter\n",
    "    res = group_or_terms(\n",
    "        [\n",
    "            (lemma, (pos, \"Lem\"))\n",
    "            for (pos, lemma) in pos_to_lemma.iteritems()\n",
    "        ] + [\n",
    "            (normalized_term, (pos, \"Norm\"))\n",
    "            for (pos, normalized_terms_set) in pos_to_normalized_term_set.iteritems()\n",
    "            for normalized_term in normalized_terms_set\n",
    "        ] + [\n",
    "            (synonym, (pos, \"Syn\"))\n",
    "            for (pos, synonyms_set) in pos_to_synset.iteritems()\n",
    "            for synonym in synonyms_set\n",
    "        ] + [\n",
    "            (noun_plural, (\"noun\", \"Plu\"))\n",
    "            for noun_plural in noun_plurals\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        (synonym, tags)\n",
    "        for (synonym, tags) in res\n",
    "        if (\" \" + term + \" \") not in (\" \" + synonym + \" \")  # remove synonyms that contains the original term\n",
    "            and is_good_expansion(synonym)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_entity_expansions(segment_id):\n",
    "    if segment_id >= len(expansion_support[\"segment_id_to_entity_id_tags_list\"]):\n",
    "        return []\n",
    "\n",
    "    res = [\n",
    "        (expansion_support[\"segment_id_to_segment\"][new_segment_id], expansion_support[\"entity_id_to_tags_segment_id_list\"][entity_id][0] + tags)\n",
    "        for entity_id, tags in expansion_support[\"segment_id_to_entity_id_tags_list\"][segment_id]\n",
    "        for new_segment_id in expansion_support[\"entity_id_to_tags_segment_id_list\"][entity_id][1]\n",
    "    ]\n",
    "\n",
    "    segment_src = expansion_support[\"segment_id_to_segment\"][segment_id]\n",
    "    return [\n",
    "        (segment, tags)\n",
    "        for segment, tags in res\n",
    "        if (\" \" + segment_src + \" \") not in (\" \" + segment + \" \")  # remove synonyms that contains the original term\n",
    "    ]\n",
    "\n",
    "def get_entity_expansions(segment):\n",
    "    segment_id = expansion_support[\"segment_to_segment_id\"].get(segment, None)\n",
    "\n",
    "    if segment_id is None:\n",
    "        if \" \" not in segment and segment in collapsed_segment_to_segment_id_list:\n",
    "            # TEMP CODE\n",
    "            return sum([\n",
    "                _get_entity_expansions(new_segment_id)\n",
    "                for new_segment_id in collapsed_segment_to_segment_id_list[segment]\n",
    "            ], [])\n",
    "\n",
    "        return []\n",
    "\n",
    "    return _get_entity_expansions(segment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REWRITING STRATEGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates the query representation without any expansion\n",
    "def query_to_base(query):\n",
    "    # normalize the text\n",
    "    query = normalize_text(query)\n",
    "\n",
    "    # remove the stop words according to if they belong to some entity or not\n",
    "    query = remove_stopwords(query, query_segmenter=None)\n",
    "\n",
    "    # tokenize the query\n",
    "    query = query.split()\n",
    "\n",
    "    # simulate the \"synset\" to match the signature. The synset is composed only by the term istelf (and its tag)\n",
    "    query = map((lambda term: [get_source_term(term)]), query)\n",
    "\n",
    "    # the expanded query is composed only by this segmentation\n",
    "    return [query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expand using the thesaurus and the entities, but segmenting the query before\n",
    "def get_query_to_segmented_thesaurus_expansion(min_segmentation_freq):\n",
    "    global segments_thesaurus, term_to_df\n",
    "    full_segmenter = PySegmenter(\n",
    "        set(\n",
    "            segment[1:-1]\n",
    "            for segment in term_to_df\n",
    "            if segment[0]==segment[-1]==\"\\\"\" and segment[1:-1] in segments_thesaurus\n",
    "        ),\n",
    "        term_to_df,\n",
    "        -1.0,\n",
    "        min_segmentation_freq\n",
    "    )\n",
    "\n",
    "    def _query_to_segm_ent_exp(query):\n",
    "        # normalize the text\n",
    "        query = normalize_text(query)\n",
    "\n",
    "        # remove the stop words according to if they belong to some entity or not\n",
    "        query = remove_stopwords(query, query_segmenter=full_segmenter)\n",
    "\n",
    "        # segment using entities and thesaurus words (the order is important)\n",
    "        query_terms = full_segmenter.segment(query)\n",
    "\n",
    "        # create synset\n",
    "        query = map(\n",
    "            (lambda t: [get_source_term(t)] + filter_expansions(group_or_terms(get_thesaurus_expansions(t) + get_entity_expansions(t)), query_terms)),\n",
    "            query_terms\n",
    "        )\n",
    "\n",
    "        # the expanded query is composed only by this segmentation\n",
    "        return [query]\n",
    "\n",
    "    return _query_to_segm_ent_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strategies = OrderedDict([\n",
    "        (\"Base\", query_to_base),\n",
    "        (\"SegmentedThesaurusExpansion(100)\", get_query_to_segmented_thesaurus_expansion(100)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_strategy_name_to_query_id_to_query = OrderedDict()\n",
    "keys = []\n",
    "table = []\n",
    "for strategy_name, strategy in strategies.iteritems():\n",
    "    all_strategy_name_to_query_id_to_query[strategy_name] = dict()\n",
    "\n",
    "    start_time = time.time()\n",
    "    for num, query_id in pb.iter_progress(enumerate(iterator), size=len(query_id_to_query), labeling_fun={\"prefix\":strategy_name}, hide_bar_on_success=True):\n",
    "        all_strategy_name_to_query_id_to_query[strategy_name][query_id] = strategy(query_id_to_query[query_id])\n",
    "    keys.append(strategy_name)\n",
    "    table.append([1.0 * (time.time()-start_time) / len(query_id_to_query)])\n",
    "# it lasts 7min 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(table, index=keys, columns=[\"Avg. expansions time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTE THE NUMBER OF MATCHES OF THE EXPANDED QUERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the number of matches of each rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "strategy_name_to_query_id_to_num_match = OrderedDict()\n",
    "\n",
    "for strategy_name in strategies:\n",
    "    strategy_name_to_query_id_to_num_match[strategy_name] = dict((query_id, 0) for query_id in query_id_to_doc_id_list)\n",
    "\n",
    "for doc_id, doc_text in pb.iter_progress(doc_id_to_text.iteritems(), size=len(doc_id_to_text)):\n",
    "    if doc_id not in doc_id_to_query_id_list:\n",
    "        continue\n",
    "    for query_id in doc_id_to_query_id_list[doc_id]:\n",
    "        # for each strategy check if the query matchs the document\n",
    "        for strategy_name, _query_id_to_query in all_strategy_name_to_query_id_to_query.iteritems():\n",
    "            strategy_name_to_query_id_to_num_match[strategy_name][query_id] += \\\n",
    "                query_match(_query_id_to_query[query_id], doc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GROUND TRUTH BUILD (using the same format used previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strategy_name = \"SegmentedThesaurusExpansion(100)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert strategy_name in strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for the training of the models we consider only the queries having at least one candidate expansion that can improve its recall.\n",
    "queries_with_recall_improvement = [\n",
    "    query_id\n",
    "    for query_id in query_id_to_num_match\n",
    "    if strategy_name_to_query_id_to_num_match[strategy_name][query_id] > strategy_name_to_query_id_to_num_match[\"Base\"][query_id]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPUTE THE WORD OCCURRENCES OF EACH QUERY, NEEDED BY THE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "def compute_word_occurrence_set(expanded_query, doc_id_list, doc_id_to_text):\n",
    "    word_set = set(\n",
    "        word_and_tags[0]\n",
    "        for and_query in expanded_query\n",
    "        for synset in and_query\n",
    "        for word_and_tags in synset\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        (word, set(doc_id\n",
    "                   for doc_id in doc_id_list\n",
    "                   if (\" \" + word + \" \") in doc_id_to_text[doc_id]))\n",
    "        for word in word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_id_to_word_to_occurrence_set = dict()\n",
    "\n",
    "for query_id in pb.iter_progress(query_id_to_query):\n",
    "    if query_id < 0:\n",
    "        continue\n",
    "    query_id_to_word_to_occurrence_set[query_id] = compute_word_occurrence_set(\n",
    "        all_strategy_name_to_query_id_to_query[strategy_name][query_id],\n",
    "        query_id_to_doc_id_list[query_id],\n",
    "        doc_id_to_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for query_id in pb.iter_progress(query_id_to_query):\n",
    "    if query_id < 0:\n",
    "        continue\n",
    "    query_id_to_word_to_occurrence_set[query_id].update(compute_word_occurrence_set(\n",
    "        all_strategy_name_to_query_id_to_query[\"Base\"][query_id],\n",
    "        query_id_to_doc_id_list[query_id],\n",
    "        doc_id_to_text\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE THE GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.isdir(cfg.processed_dir + \"training/\"):\n",
    "    os.mkdir(cfg.processed_dir + \"training/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(cfg.processed_dir + \"training/expanded_query.queries_with_recall_improvement.pickle\", \"wb\") as outfile:\n",
    "    cPickle.dump(queries_with_recall_improvement, outfile, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(cfg.processed_dir + \"training/expanded_query.query_id_to_word_to_occurrence_set.pickle\", \"wb\") as outfile:\n",
    "    cPickle.dump(query_id_to_word_to_occurrence_set, outfile, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(cfg.processed_dir + \"training/query_id_to_expanded_query.pickle\", \"wb\") as outfile:\n",
    "    cPickle.dump(\n",
    "        dict((query_id, expanded_query) for query_id, expanded_query in all_strategy_name_to_query_id_to_query[strategy_name].iteritems() if query_id in query_id_to_query),\n",
    "        outfile,\n",
    "        protocol=cPickle.HIGHEST_PROTOCOL\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(cfg.processed_dir + \"training/query_id_to_base_query.pickle\", \"wb\") as outfile:\n",
    "    cPickle.dump(\n",
    "        dict((query_id, expanded_query) for query_id, expanded_query in all_strategy_name_to_query_id_to_query[\"Base\"].iteritems() if query_id in query_id_to_query),\n",
    "        outfile,\n",
    "        protocol=cPickle.HIGHEST_PROTOCOL\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {
    "8f4e9f1abacd4d9592e211e9756dce5f": {
     "views": [
      {
       "cell_index": 40
      }
     ]
    },
    "ca17dc7e952a421fa5c9134752edaedf": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
